{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70776ba4-34a5-41b1-af8f-b3fa99e8cdc1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#  **1.1 – Cos’è un LLM via API**\n",
    "\n",
    "---\n",
    "\n",
    "##  Obiettivo:\n",
    "\n",
    "Capire cosa significa \"usare un LLM via API\", a cosa serve, e perché è oggi uno standard nell’integrazione dei modelli linguistici nei progetti reali.\n",
    "\n",
    "---\n",
    "\n",
    "##  Cos’è un LLM (Large Language Model)?\n",
    "\n",
    "Un **Large Language Model (LLM)** è un modello di intelligenza artificiale addestrato su enormi quantità di testo per:\n",
    "\n",
    "* Comprendere e generare linguaggio naturale\n",
    "* Completare frasi, rispondere a domande, scrivere articoli, riassunti, codice, ecc.\n",
    "* Supportare casi d’uso come chatbot, assistenti, scrittura, ricerca semantica\n",
    "\n",
    "Esempi: **GPT-4**, **Claude**, **Mistral**, **LLaMA**, **Gemini**\n",
    "\n",
    "---\n",
    "\n",
    "##  Cos’è un’API?\n",
    "\n",
    "Una **API (Application Programming Interface)** è un’interfaccia che permette a un programma di “parlare” con un altro, spesso tramite internet.\n",
    "\n",
    "Nel nostro contesto:\n",
    "\n",
    "* Tu invii una richiesta HTTP con un testo (prompt)\n",
    "* Il server risponde con una risposta generata dal modello (output)\n",
    "\n",
    " Non devi installare o addestrare nulla. L’LLM è *hosted* dal provider.\n",
    "\n",
    "---\n",
    "\n",
    "##  Perché usare un LLM via API?\n",
    "\n",
    "| Vantaggio                     | Spiegazione                                         |\n",
    "| ----------------------------- | --------------------------------------------------- |\n",
    "|  Sempre aggiornato          | Non devi gestire update o versioni                  |\n",
    "|  Prestazioni elevate         | I server del provider sono ottimizzati              |\n",
    "|  Sicurezza e autenticazione | Accesso controllato con API key                     |\n",
    "|  Modello a consumo          | Paghi solo per ciò che usi                          |\n",
    "|  Scalabilità                | Puoi usarlo da un’app, un sito, un software desktop |\n",
    "\n",
    "---\n",
    "\n",
    "##  Differenze tra LLM “locale” e “via API”\n",
    "\n",
    "| Caratteristica | LLM via API           | LLM locale (offline)      |\n",
    "| -------------- | --------------------- | ------------------------- |\n",
    "| Setup          | Nessuna installazione | Richiede GPU, RAM, setup  |\n",
    "| Costi iniziali | Nessuno               | Elevati (hardware, tempo) |\n",
    "| Privacy        | Va gestita            | Tutto resta in locale     |\n",
    "| Scalabilità    | Altissima             | Limitata a risorse locali |\n",
    "| Aggiornamenti  | Automatici            | Manuali                   |\n",
    "\n",
    "---\n",
    "\n",
    "##  Come funziona in pratica\n",
    "\n",
    "### Esempio concettuale:\n",
    "\n",
    "1. Scriviamo un prompt:\n",
    "   `“Scrivi un titolo SEO per un articolo su Firenze”`\n",
    "\n",
    "2. Lo inviamo via HTTP POST a un endpoint del provider (es. Azure, Hugging Face)\n",
    "\n",
    "3. L’LLM elabora il prompt\n",
    "\n",
    "4. Risponde con:\n",
    "   `\"Firenze: la guida completa tra arte, cibo e panorami mozzafiato\"`\n",
    "\n",
    "---\n",
    "\n",
    "##  Mini esempio in Python (simulato)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1680e5b-facc-4263-a4d5-d4613ccd8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"your_api_key\"\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "body = {\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Spiegami cosa fa un LLM via API in parole semplici\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=body)\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66beb6db-ff5d-4f13-90ac-f4fb14dc2c3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Concetto chiave\n",
    "\n",
    "> **Usare un LLM via API significa “delegare” la generazione del testo a un sistema esterno accessibile via internet, che esegue l’elaborazione per noi e ci restituisce il risultato.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388dea2d-5f15-43c8-8e93-9bd2be1aa79e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 1.2 – Cos'è un'inferenza via API\n",
    "\n",
    "---\n",
    "\n",
    "## Definizione\n",
    "\n",
    "Nel contesto dell’intelligenza artificiale, il termine **inferenza** si riferisce al processo attraverso cui un modello addestrato riceve un input e produce un output.\n",
    "\n",
    "Nel caso dei **Large Language Models (LLM)**, l’inferenza consiste nell’elaborazione di un **testo in ingresso (prompt)** per generare un **testo in uscita** coerente, pertinente e plausibile.\n",
    "\n",
    "Quando questa inferenza avviene tramite **API**, significa che il modello non gira in locale ma su un **server remoto**, accessibile tramite chiamate HTTP. Tu invii una richiesta con dei parametri, e ricevi la risposta del modello in formato JSON.\n",
    "\n",
    "---\n",
    "\n",
    "## Anatomia di una richiesta di inferenza\n",
    "\n",
    "Una tipica richiesta di inferenza via API contiene:\n",
    "\n",
    "1. **Endpoint URL**: è l'indirizzo del server a cui ci si connette (es. `https://api.openai.com/v1/chat/completions`)\n",
    "2. **Header HTTP**:\n",
    "\n",
    "   * `Authorization`: la chiave API per l’autenticazione\n",
    "   * `Content-Type`: il formato del contenuto (tipicamente `application/json`)\n",
    "3. **Corpo della richiesta (body)**: i parametri da inviare al modello, tra cui:\n",
    "\n",
    "   * il modello da usare (es. `gpt-4`)\n",
    "   * il testo in input (prompt, oppure `messages` nel caso del formato chat)\n",
    "   * i parametri di controllo della generazione (es. `temperature`, `max_tokens`, `top_p`)\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio concreto\n",
    "\n",
    "Ecco un esempio semplificato di richiesta POST a un LLM via API:\n",
    "\n",
    "```json\n",
    "POST https://api.openai.com/v1/completions\n",
    "\n",
    "Header:\n",
    "{\n",
    "  \"Authorization\": \"Bearer your_api_key\",\n",
    "  \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "Body:\n",
    "{\n",
    "  \"model\": \"text-davinci-003\",\n",
    "  \"prompt\": \"Cos'è l'inferenza in un LLM?\",\n",
    "  \"temperature\": 0.7,\n",
    "  \"max_tokens\": 150\n",
    "}\n",
    "```\n",
    "\n",
    "Il server risponderà con un JSON simile a:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"text\": \"L'inferenza è il processo attraverso cui un modello di linguaggio genera un testo...\",\n",
    "      ...\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Input → Elaborazione → Output\n",
    "\n",
    "* **Input**: un testo (prompt) o una conversazione (array di messaggi)\n",
    "* **Elaborazione**: il modello genera predizione token per token, scegliendo la parola successiva più probabile\n",
    "* **Output**: il testo finale viene restituito nel corpo della risposta\n",
    "\n",
    "---\n",
    "\n",
    "## Cosa succede internamente\n",
    "\n",
    "Durante una chiamata di inferenza:\n",
    "\n",
    "1. Il server riceve la richiesta e autentica l'API key\n",
    "2. Valida il formato e i parametri (es. controlla che `max_tokens` sia ammesso)\n",
    "3. Passa il prompt al modello, che lo tokenizza (trasforma in numeri)\n",
    "4. Il modello genera gli output token uno alla volta fino al raggiungimento del `max_tokens` o di un segnale di fine\n",
    "5. L’output viene ritrasformato in testo e inviato al client in una risposta HTTP\n",
    "\n",
    "Questo processo è invisibile all’utente ma ha un costo computazionale e viene conteggiato in **token**, non in caratteri.\n",
    "\n",
    "---\n",
    "\n",
    "## Token e inferenza\n",
    "\n",
    "È importante comprendere che il costo e la latenza dell’inferenza dipendono da:\n",
    "\n",
    "* **Numero di token in input**: più lungo è il prompt, più tempo servirà a processarlo\n",
    "* **Numero di token in output**: ogni token generato è una nuova inferenza iterativa\n",
    "* **Complessità del modello**: GPT-4 richiede più tempo di GPT-3.5 per generare la stessa frase\n",
    "* **Temperature e randomness**: una temperatura più alta può far esplorare più opzioni e quindi variare la performance\n",
    "\n",
    "---\n",
    "\n",
    "## Tipi di inferenza via API\n",
    "\n",
    "Esistono diversi **tipi di inferenza** che si distinguono per formato e utilizzo:\n",
    "\n",
    "| Tipo inferenza | Input richiesto             | Output restituito             | Esempi d’uso                      |\n",
    "| -------------- | --------------------------- | ----------------------------- | --------------------------------- |\n",
    "| Completion     | Testo libero (prompt)       | Completamento testuale        | Copywriting, email, articoli      |\n",
    "| Chat           | Serie di messaggi           | Risposta del modello          | Chatbot, agenti, interfacce umane |\n",
    "| Embedding      | Frasi o documenti singoli   | Vettori numerici              | Ricerca semantica, clustering     |\n",
    "| Code           | Prompt di codice o commenti | Codice o spiegazioni generate | Generazione codice, refactoring   |\n",
    "\n",
    "---\n",
    "\n",
    "## Aspetti critici da gestire\n",
    "\n",
    "1. **Controllo dell’output**: l’inferenza può variare a ogni chiamata. Per ottenere output coerenti serve un prompt ben strutturato e testato.\n",
    "2. **Ripetibilità**: puoi forzarla impostando `temperature=0` e `seed` se supportato (alcuni modelli non hanno ancora il controllo seed).\n",
    "3. **Tempo di risposta (latency)**: è influenzato dal numero di token, dal carico sul server e dal tipo di modello scelto.\n",
    "4. **Costi**: ogni inferenza ha un costo calcolato sul numero di token elaborati (vedremo nel dettaglio nella sezione sui costi).\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* L’inferenza via API è il modo in cui un’applicazione interagisce con un LLM.\n",
    "* Consiste nell’inviare un input testuale e ottenere un output generato.\n",
    "* L’intero processo è governato da parametri che influenzano l’output finale.\n",
    "* È fondamentale per creare applicazioni AI scalabili e integrate.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230ad1d-201e-401c-a961-ccb793ff53db",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 2.1 – Endpoint \"chat\"\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivo della sezione\n",
    "\n",
    "Capire come funziona l’**endpoint `chat`**, perché è oggi lo standard per lavorare con LLM moderni (come GPT-3.5, GPT-4, Claude, ecc.), quali sono le sue caratteristiche, e come costruire correttamente i messaggi per controllare il comportamento del modello.\n",
    "\n",
    "---\n",
    "\n",
    "## Cosa si intende per \"chat\"\n",
    "\n",
    "L’**endpoint chat** è un’interfaccia pensata per simulare una conversazione tra:\n",
    "\n",
    "* l’utente (user),\n",
    "* il modello AI (assistant),\n",
    "* ed eventualmente un sistema (system) che imposta le istruzioni.\n",
    "\n",
    "Non è limitato alle chatbot: è usato in una vasta gamma di applicazioni, dalla generazione di codice alla scrittura creativa, proprio grazie al **formato strutturato** dei messaggi.\n",
    "\n",
    "---\n",
    "\n",
    "## Struttura dell’input: array di messaggi\n",
    "\n",
    "L’input di una chiamata all’endpoint chat è un **array di messaggi** (`messages`), dove ogni messaggio ha due proprietà fondamentali:\n",
    "\n",
    "```json\n",
    "{\"role\": \"<ruolo>\", \"content\": \"<testo>\"}\n",
    "```\n",
    "\n",
    "I ruoli possibili sono:\n",
    "\n",
    "| Ruolo       | Descrizione                                                   |\n",
    "| ----------- | ------------------------------------------------------------- |\n",
    "| `system`    | Instruisce il modello su come comportarsi (contesto iniziale) |\n",
    "| `user`      | Messaggi inviati dall’utente                                  |\n",
    "| `assistant` | Risposte precedenti del modello (possono essere simulate)     |\n",
    "\n",
    "---\n",
    "\n",
    "### Esempio di conversazione semplice\n",
    "\n",
    "```json\n",
    "\"messages\": [\n",
    "  {\"role\": \"system\", \"content\": \"Sei un assistente esperto di finanza aziendale.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Quali sono le differenze tra ROI e ROE?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Il modello risponderà come se fosse un consulente finanziario, perché lo abbiamo “instruito” con il messaggio `system`.\n",
    "\n",
    "---\n",
    "\n",
    "## A cosa serve il messaggio `system`\n",
    "\n",
    "Il messaggio `system` non viene mai mostrato all’utente finale, ma **è il modo principale per controllare il comportamento del modello**.\n",
    "\n",
    "### Alcuni esempi pratici:\n",
    "\n",
    "| Prompt                                                         | Effetto                  |\n",
    "| -------------------------------------------------------------- | ------------------------ |\n",
    "| `\"Sei un esperto copywriter che scrive in tono professionale\"` | Generazione SEO coerente |\n",
    "| `\"Rispondi solo con dati verificabili e numerati\"`             | Risposte strutturate     |\n",
    "| `\"Limita la risposta a massimo 3 frasi\"`                       | Risposte concise         |\n",
    "| `\"Parla come uno youtuber entusiasta\"`                         | Cambia il tono           |\n",
    "\n",
    "La sua efficacia dipende dal modello: GPT-4 lo segue meglio di GPT-3.5, per esempio.\n",
    "\n",
    "---\n",
    "\n",
    "## Parametri avanzati dell’endpoint chat\n",
    "\n",
    "Oltre a `messages`, si possono aggiungere altri parametri per personalizzare l’inferenza:\n",
    "\n",
    "| Parametro           | Descrizione                                          |\n",
    "| ------------------- | ---------------------------------------------------- |\n",
    "| `temperature`       | Grado di creatività. 0 = deterministico, 1 = casuale |\n",
    "| `top_p`             | Nucleus sampling. Alternativa a temperature          |\n",
    "| `max_tokens`        | Massimo numero di token in output                    |\n",
    "| `presence_penalty`  | Penalizza la ripetizione di concetti già menzionati  |\n",
    "| `frequency_penalty` | Penalizza parole già usate frequentemente            |\n",
    "\n",
    "Un esempio avanzato:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"model\": \"gpt-4\",\n",
    "  \"messages\": [...],\n",
    "  \"temperature\": 0.7,\n",
    "  \"max_tokens\": 500,\n",
    "  \"presence_penalty\": 0.5,\n",
    "  \"frequency_penalty\": 0.5\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Come costruire un’interazione multi-turno\n",
    "\n",
    "Il valore dell’endpoint chat si vede nelle **interazioni multi-turno**, ovvero le conversazioni con contesto.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"role\": \"system\", \"content\": \"Agisci come un esperto legale italiano.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Qual è la differenza tra società semplice e SRL?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"La società semplice è una forma ...\"},\n",
    "  {\"role\": \"user\", \"content\": \"E in quale conviene iniziare se ho pochi clienti?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Il modello capisce il contesto della conversazione e genera risposte coerenti con le domande precedenti.\n",
    "\n",
    "---\n",
    "\n",
    "## Come simulare un dialogo “fittizio” per controllare il modello\n",
    "\n",
    "È possibile **inserire risposte simulate dell’assistente** per pilotare il comportamento.\n",
    "\n",
    "Esempio:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\"role\": \"system\", \"content\": \"Sei un assistente motivazionale.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Non ho voglia di lavorare.\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"Capisco. A volte serve solo un piccolo passo per ripartire.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hai ragione. Dammi un consiglio.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "Anche se la conversazione è tutta costruita, il modello risponderà come se stesse continuando un dialogo reale.\n",
    "\n",
    "---\n",
    "\n",
    "## Codice Python di esempio\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Sei un esperto SEO e scrivi in tono professionale.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Scrivi un'introduzione per un articolo su eventi in montagna.\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* L’endpoint `chat` è il formato consigliato per interagire con i LLM moderni.\n",
    "* Il prompt è composto da un array di messaggi con ruoli distinti.\n",
    "* Il `system` prompt consente di controllare tono, stile e ruolo del modello.\n",
    "* Le conversazioni multi-turno permettono di costruire interazioni più sofisticate e personalizzate.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acdec28-2dd3-4692-98dd-96d261a50af0",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 2.2 – Endpoint \"completions\"\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivo della sezione\n",
    "\n",
    "Comprendere il funzionamento dell’**endpoint completions**, le sue differenze rispetto all’endpoint chat, i casi d’uso in cui è preferibile, e come scrivere prompt efficaci per ottenere output di qualità.\n",
    "\n",
    "---\n",
    "\n",
    "## Che cos’è l’endpoint completions\n",
    "\n",
    "L’**endpoint `completions`** è il metodo più classico e diretto per interagire con un LLM: fornisci un **testo grezzo (prompt)** e il modello lo completa.\n",
    "\n",
    "Si basa su un paradigma semplice:\n",
    "\n",
    "> “Data una sequenza iniziale di testo, continua nel modo più probabile e coerente.”\n",
    "\n",
    "È stato il primo tipo di interfaccia messo a disposizione dagli LLM (es. GPT-2, GPT-3).\n",
    "\n",
    "---\n",
    "\n",
    "## Differenza chiave con l’endpoint `chat`\n",
    "\n",
    "| Aspetto                 | Endpoint `completions`        | Endpoint `chat`                       |\n",
    "| ----------------------- | ----------------------------- | ------------------------------------- |\n",
    "| Struttura               | Prompt testuale grezzo        | Serie strutturata di messaggi         |\n",
    "| Controllo del tono      | Richiede prompt complesso     | Più semplice tramite `system`         |\n",
    "| Contesto multi-turno    | Manuale (devi scrivere tutto) | Integrato nella conversazione         |\n",
    "| Flessibilità sintattica | Maggiore libertà              | Struttura imposta (`role`, `content`) |\n",
    "| Modelli supportati      | GPT-3, Codex, text-davinci-\\* | GPT-3.5-turbo, GPT-4, Claude, ecc.    |\n",
    "\n",
    "---\n",
    "\n",
    "## Come funziona: prompt → completamento\n",
    "\n",
    "Tu fornisci un prompt come:\n",
    "\n",
    "```\n",
    "Scrivi un'introduzione per un articolo sul turismo in Toscana.\n",
    "```\n",
    "\n",
    "Il modello risponde continuando:\n",
    "\n",
    "```\n",
    "La Toscana è una delle regioni più affascinanti d’Italia, grazie alla sua combinazione unica di arte, natura e tradizioni enogastronomiche...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prompt engineering nell’endpoint completions\n",
    "\n",
    "Poiché il prompt non è strutturato, è **fondamentale imparare a scriverlo bene**.\n",
    "Alcuni pattern utili:\n",
    "\n",
    "### 1. Prompt con istruzione esplicita\n",
    "\n",
    "```\n",
    "Scrivi una breve descrizione del prodotto seguente:\n",
    "Prodotto: Spazzolino elettrico a ultrasuoni\n",
    "Descrizione:\n",
    "```\n",
    "\n",
    "### 2. Prompt con esempio (few-shot)\n",
    "\n",
    "```\n",
    "Q: Qual è la capitale della Francia?\n",
    "A: Parigi\n",
    "\n",
    "Q: Qual è la capitale della Germania?\n",
    "A:\n",
    "```\n",
    "\n",
    "### 3. Prompt con template\n",
    "\n",
    "```\n",
    "Titolo: 5 motivi per visitare la Toscana\n",
    "Paragrafo introduttivo:\n",
    "```\n",
    "\n",
    "Più il prompt è chiaro e strutturato, migliore sarà l’output.\n",
    "\n",
    "---\n",
    "\n",
    "## Parametri principali\n",
    "\n",
    "I parametri usati nell’endpoint completions sono simili a quelli dell’endpoint chat:\n",
    "\n",
    "| Parametro           | Descrizione                      |\n",
    "| ------------------- | -------------------------------- |\n",
    "| `model`             | Es. `\"text-davinci-003\"`         |\n",
    "| `prompt`            | Testo da completare              |\n",
    "| `temperature`       | Creatività: da 0 a 1             |\n",
    "| `top_p`             | Alternativa alla temperatura     |\n",
    "| `max_tokens`        | Numero massimo di token generati |\n",
    "| `stop`              | Sequenza di arresto anticipato   |\n",
    "| `presence_penalty`  | Penalizza argomenti già trattati |\n",
    "| `frequency_penalty` | Penalizza parole ripetute        |\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio base in Python\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Scrivi una descrizione di un prodotto per un hotel di lusso in Toscana.\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio con sequenza di stop\n",
    "\n",
    "```python\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Elenca tre vantaggi dell'utilizzo di un assistente AI:\\n1.\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    "    stop=[\"\\n\"]\n",
    ")\n",
    "```\n",
    "\n",
    "Questo tipo di prompt viene usato spesso per generare output a elenco o singola riga.\n",
    "\n",
    "---\n",
    "\n",
    "## Limiti e difficoltà\n",
    "\n",
    "1. **Nessun supporto nativo per il dialogo**: per mantenere il contesto, devi concatenare manualmente i prompt e le risposte precedenti.\n",
    "2. **Controllo del tono e del ruolo più difficile**: richiede prompt molto articolati.\n",
    "3. **Deprecazione progressiva**: molti provider stanno passando all’interfaccia chat anche per usi classici.\n",
    "\n",
    "---\n",
    "\n",
    "## Quando usare completions invece di chat\n",
    "\n",
    "* Se vuoi massima libertà sul formato del prompt\n",
    "* Quando usi modelli legacy o pre-addestrati (es. Codex per codice)\n",
    "* In flussi con prompt complessi e molto controllati\n",
    "* Per attività batch, riempimento di template, generazione automatica di contenuti statici\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* L’endpoint completions è basato su un prompt grezzo che viene completato dal modello.\n",
    "* Offre molta flessibilità ma richiede maggiore cura nella scrittura dei prompt.\n",
    "* È adatto per attività singole, statiche o ad alto controllo sintattico.\n",
    "* Nei casi più moderni, l’endpoint `chat` è più semplice e potente per interazioni conversazionali.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb347e0-f1d9-46f4-9e02-15eee3a453df",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Confronto diretto: `chat` vs `completions`\n",
    "\n",
    "---\n",
    "\n",
    "##  Task comune: generare un’introduzione per un articolo SEO sul tema:\n",
    "\n",
    "> *“Eventi in montagna: guida completa per il 2025”*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Con approccio `completions`\n",
    "\n",
    "### Prompt testuale grezzo:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Scrivi un’introduzione SEO-friendly per un articolo intitolato:\n",
    "“Eventi in montagna: guida completa per il 2025”.\n",
    "Usa uno stile informativo, coinvolgente e professionale.\n",
    "Lunghezza: 3-4 frasi.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### Codice:\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prompt,\n",
    "    temperature=0.7,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"].strip())\n",
    "```\n",
    "\n",
    "### Vantaggi:\n",
    "\n",
    "* Nessuna struttura imposta: massimo controllo sulla forma del prompt\n",
    "* Comportamento più “grezzo”, utile se si vuole un output molto guidato\n",
    "\n",
    "### Svantaggi:\n",
    "\n",
    "* Più suscettibile a errori se il prompt non è ben progettato\n",
    "* Difficile estendere a multi-turno o a dialogo interattivo\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Con approccio `chat`\n",
    "\n",
    "### Prompt strutturato:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Sei un copywriter SEO esperto in articoli per il web. Scrivi in modo professionale e coinvolgente.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Scrivi un’introduzione SEO per un articolo dal titolo: 'Eventi in montagna: guida completa per il 2025'. Deve essere informativa e accattivante, lunga circa 3-4 frasi.\"}\n",
    "]\n",
    "```\n",
    "\n",
    "### Codice:\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"].strip())\n",
    "```\n",
    "\n",
    "### Vantaggi:\n",
    "\n",
    "* Miglior controllo sul ruolo e sul tono tramite `system` message\n",
    "* Più robusto a prompt ambigui\n",
    "* Ottimo per progetti che evolvono in multi-turno (es. app, chatbot, strumenti interattivi)\n",
    "\n",
    "### Svantaggi:\n",
    "\n",
    "* Leggermente più verboso nella sintassi\n",
    "* Richiede familiarità con il formato `messages`\n",
    "\n",
    "---\n",
    "\n",
    "## Risultati attesi\n",
    "\n",
    "Con entrambi gli approcci, otterrai un paragrafo simile, ma:\n",
    "\n",
    "* Con `completions`, la qualità dipende totalmente dal wording del prompt\n",
    "* Con `chat`, il `system` message offre un **contesto persistente**, che può migliorare coerenza e stile\n",
    "\n",
    "---\n",
    "\n",
    "## Quando scegliere l’uno o l’altro\n",
    "\n",
    "| Scenario                                        | Consigliato |\n",
    "| ----------------------------------------------- | ----------- |\n",
    "| Prompt molto specifici, su un’unica riga        | completions |\n",
    "| Prompt lunghi e strutturati                     | chat        |\n",
    "| Generazione autonoma e ripetibile               | completions |\n",
    "| Conversazioni multi-turno o con memoria         | chat        |\n",
    "| Modelli moderni come GPT-4, Claude, Gemini      | chat        |\n",
    "| Compatibilità con vecchi modelli (Codex, GPT-3) | completions |\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusione\n",
    "\n",
    "La **differenza chiave** non è solo sintattica ma **concettuale**:\n",
    "\n",
    "* `completions` simula il completamento di testo, adatto a prompt “autonomi”.\n",
    "* `chat` simula un assistente, adatto a interazioni strutturate e scalabili.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b1235-aebf-47ad-ab34-f83c33c57340",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 2.3 – Endpoint \"embeddings\"\n",
    "\n",
    "---\n",
    "\n",
    "## Obiettivo della sezione\n",
    "\n",
    "Capire cosa sono gli embeddings generati dagli LLM, come funziona l’**endpoint embeddings** via API, perché sono diversi dalla generazione testuale e come usarli per potenziare applicazioni di ricerca e intelligenza semantica.\n",
    "\n",
    "---\n",
    "\n",
    "## Che cos’è un embedding?\n",
    "\n",
    "Un **embedding** è una **rappresentazione numerica vettoriale** di un’informazione testuale.\n",
    "\n",
    "In pratica:\n",
    "\n",
    "* Una frase → viene trasformata in un vettore di numeri\n",
    "* Questo vettore rappresenta il “significato” del testo in uno spazio multidimensionale\n",
    "* Testi simili avranno vettori vicini (alta similarità), testi diversi avranno vettori distanti\n",
    "\n",
    "> Non serve generare testo, ma \"mappare\" semanticamente contenuti.\n",
    "\n",
    "---\n",
    "\n",
    "## A cosa servono gli embeddings?\n",
    "\n",
    "Gli embeddings sono usati per:\n",
    "\n",
    "| Caso d’uso                           | Descrizione                                                                         |\n",
    "| ------------------------------------ | ----------------------------------------------------------------------------------- |\n",
    "| Ricerca semantica                    | Trovare documenti/frasi simili a una query anche se non contengono le stesse parole |\n",
    "| RAG (Retrieval Augmented Generation) | Estrarre i testi più rilevanti da fornire a un LLM per rispondere                   |\n",
    "| Clustering                           | Raggruppare documenti con contenuto simile                                          |\n",
    "| Classificazione                      | Usarli come feature in un modello ML                                                |\n",
    "| Recommendation                       | Trovare item simili (es. prodotti, film, articoli)                                  |\n",
    "\n",
    "---\n",
    "\n",
    "## Come funziona l’endpoint embeddings via API\n",
    "\n",
    "* **Input**: una o più frasi/testi/documenti\n",
    "* **Output**: un vettore (array di numeri) per ogni input\n",
    "* **Modello**: specifico per embeddings (es. `text-embedding-ada-002` su OpenAI)\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio base in Python (OpenAI)\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "response = openai.Embedding.create(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    input=[\n",
    "        \"Eventi in montagna nel 2025\",\n",
    "        \"Guida turistica per la montagna\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "for item in response[\"data\"]:\n",
    "    print(f\"Index {item['index']}: embedding length = {len(item['embedding'])}\")\n",
    "```\n",
    "\n",
    "L’output è un dizionario con:\n",
    "\n",
    "* `index`: posizione del testo nell’input\n",
    "* `embedding`: lista di float (es. 1536 valori)\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio Hugging Face (Inference API)\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "headers = {\"Authorization\": f\"Bearer hf_your_token\"}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json={\"inputs\": \"Eventi in montagna nel 2025\"})\n",
    "embedding = response.json()\n",
    "\n",
    "print(len(embedding[0]))  # Lunghezza del vettore\n",
    "```\n",
    "\n",
    "> Hugging Face offre diversi modelli con lunghezze diverse (384, 768, 1024...).\n",
    "\n",
    "---\n",
    "\n",
    "## Similarità tra due testi\n",
    "\n",
    "Una volta ottenuti due embeddings, possiamo misurarne la **similarità** (quanto sono semanticamente vicini) usando la **cosine similarity**.\n",
    "\n",
    "### Esempio con `sklearn`:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "sim = cosine_similarity([embedding1], [embedding2])\n",
    "print(f\"Similarità: {sim[0][0]}\")\n",
    "```\n",
    "\n",
    "Il risultato sarà tra **-1 e 1**:\n",
    "\n",
    "* 1 = significato identico\n",
    "* 0 = significato non correlato\n",
    "* < 0 = significato opposto (raro)\n",
    "\n",
    "---\n",
    "\n",
    "## Considerazioni pratiche\n",
    "\n",
    "| Aspetto     | Valore                                                                             |\n",
    "| ----------- | ---------------------------------------------------------------------------------- |\n",
    "| Token input | Gli embeddings hanno limite di token (es. 8191 token per `text-embedding-ada-002`) |\n",
    "| Output      | Vettori di dimensione fissa (es. 1536)                                             |\n",
    "| Performance | Più veloci delle completion/chat                                                   |\n",
    "| Costo       | Più economici rispetto alla generazione testuale                                   |\n",
    "| Storage     | Puoi salvarli in DB, CSV, o vector DB (es. FAISS, Weaviate, Pinecone)              |\n",
    "\n",
    "---\n",
    "\n",
    "## Applicazioni reali con embeddings\n",
    "\n",
    "1. **Motore di ricerca semantico**:\n",
    "\n",
    "   * Indicizziamo contenuti (FAQ, articoli, documenti)\n",
    "   * L’utente inserisce una query\n",
    "   * Generiamo l’embedding della query\n",
    "   * Calcoliamo la similarità con tutti gli embeddings del database\n",
    "   * Selezioniamo quelli più rilevanti\n",
    "\n",
    "2. **RAG (Retrieval-Augmented Generation)**:\n",
    "\n",
    "   * Si usa l’embedding per cercare nel database le parti più pertinenti\n",
    "   * Si passa quel contenuto come contesto a GPT per generare la risposta\n",
    "\n",
    "3. **Clustering clienti o recensioni**:\n",
    "\n",
    "   * Applichiamo `KMeans` sugli embeddings\n",
    "   * Raggruppiamo in “personas” o categorie\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* L’endpoint embeddings è uno strumento fondamentale per analizzare **il significato** dei testi.\n",
    "* Permette confronti semantici molto più potenti di un semplice matching di parole.\n",
    "* È più veloce ed economico della generazione testuale, ed è usato in ogni sistema moderno di ricerca intelligente.\n",
    "* Lavorare con embeddings richiede solo il primo step di AI (trasformazione), poi si può usare ML classico o logica di confronto.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "046da5a2-04dc-4fab-9f79-a9b71a051f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarità semantica tra le frasi: 0.6567\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# 1) Inizializza il modello (solo CPU) - veloce e multilingue\n",
    "# Buon compromesso qualità/velocità per IT: paraphrase-multilingual-MiniLM-L12-v2\n",
    "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "\n",
    "# 2) Frasi da confrontare\n",
    "sentence_1 = \"Eventi in montagna nel 2025\"\n",
    "sentence_2 = \"Cosa fare sulle Alpi\"\n",
    "\n",
    "# 3) Similarità coseno con embedding normalizzati (più stabile)\n",
    "def semantic_similarity(sent1: str, sent2: str, model: SentenceTransformer) -> float:\n",
    "    emb = model.encode([sent1, sent2], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    # Con vettori unitari il coseno è il prodotto scalare\n",
    "    return float(np.dot(emb[0], emb[1]))\n",
    "\n",
    "# 4) Calcolo e stampa del risultato\n",
    "score = semantic_similarity(sentence_1, sentence_2, model)\n",
    "print(f\"Similarità semantica tra le frasi: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990b32ae-c0ba-4112-9dd1-3ff74b0d98a6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Cos’è un modello di embedding\n",
    "\n",
    "Un **modello di embedding** è una rete neurale addestrata a trasformare parole, frasi o documenti in **vettori numerici** in uno **spazio continuo multidimensionale**.\n",
    "\n",
    "* Ogni parola (o frase) non è più rappresentata come una sequenza di caratteri, ma come un punto nello spazio.\n",
    "* Parole/frasi con significato simile finiscono vicine, quelle con significato diverso finiscono lontane.\n",
    "\n",
    "Questa rappresentazione è detta **spazio semantico**.\n",
    "\n",
    "---\n",
    "\n",
    "# Come viene creato un embedding model\n",
    "\n",
    "Esistono vari approcci storici ed evolutivi:\n",
    "\n",
    "### 1. Word2Vec (2013)\n",
    "\n",
    "* Un modello semplice che impara le rappresentazioni delle **singole parole**.\n",
    "* Due varianti principali:\n",
    "\n",
    "  * **CBOW** (Continuous Bag of Words): predice una parola dato il contesto\n",
    "  * **Skip-gram**: predice il contesto dato una parola\n",
    "* Idea: se due parole appaiono in contesti simili, hanno significato simile.\n",
    "  Ad esempio: “gatto” e “cane” compaiono entrambi vicino a “animale”, “casa”, “giocare”.\n",
    "\n",
    "### 2. GloVe (2014)\n",
    "\n",
    "* Basato sulle co-occorrenze statistiche in un corpus.\n",
    "* Anche qui ogni parola ha un singolo embedding fisso.\n",
    "\n",
    "### 3. Modelli contestuali (dal 2018 in poi, es. BERT, RoBERTa, DistilBERT, e oggi modelli LLM come GPT)\n",
    "\n",
    "* Una stessa parola può avere embedding **diversi** a seconda della frase.\n",
    "* Questo risolve il problema delle **parole polisemiche** (uguali nella forma, diverse nel significato).\n",
    "\n",
    "---\n",
    "\n",
    "# Come funziona l’addestramento\n",
    "\n",
    "1. **Dataset**: grandi quantità di testo (Wikipedia, libri, articoli, web).\n",
    "2. **Tokenizzazione**: il testo viene spezzato in token (parole o sottoparole).\n",
    "3. **Obiettivo di training**:\n",
    "\n",
    "   * Predire una parola mancante dal contesto\n",
    "   * Predire il contesto di una parola\n",
    "   * Predire se due frasi sono semanticamente correlate\n",
    "4. **Backpropagation**: il modello regola i pesi interni in modo che testi simili abbiano rappresentazioni vettoriali vicine.\n",
    "5. **Output finale**: per ogni parola o frase, un vettore di dimensione fissa (es. 384, 768, 1536).\n",
    "\n",
    "---\n",
    "\n",
    "# Come si misura la similarità\n",
    "\n",
    "Gli embeddings si confrontano con misure di distanza o somiglianza:\n",
    "\n",
    "* **Cosine similarity**: misura l’angolo tra due vettori (0 = ortogonali, 1 = identici).\n",
    "* **Euclidean distance**: distanza geometrica tra i punti.\n",
    "* **Dot product**: prodotto scalare, spesso usato nei transformer.\n",
    "\n",
    "Se due frasi hanno embedding molto simili, il modello considera i loro significati correlati.\n",
    "\n",
    "---\n",
    "\n",
    "# Gestione delle parole uguali con significati diversi\n",
    "\n",
    "Questo è il problema della **polisemia** (es. “banca” = istituto finanziario oppure “banca” = panchina).\n",
    "\n",
    "I vecchi modelli (Word2Vec, GloVe) assegnavano **un unico embedding fisso** a ciascuna parola → fallivano su queste ambiguità.\n",
    "\n",
    "I modelli moderni (BERT, GPT, Sentence Transformers, ecc.) risolvono così:\n",
    "\n",
    "* L’embedding non è legato alla parola isolata, ma al **contesto intero della frase**.\n",
    "* Esempio:\n",
    "\n",
    "  * Input: “Sono andato in **banca** a depositare dei soldi”\n",
    "    → embedding vicino a “istituto finanziario”.\n",
    "  * Input: “Ci siamo seduti sulla **banca** al parco”\n",
    "    → embedding vicino a “panchina, sedile”.\n",
    "* Questo avviene grazie all’**attenzione nei transformer**, che lega ogni parola al contesto circostante.\n",
    "\n",
    "---\n",
    "\n",
    "# Riassumendo\n",
    "\n",
    "1. Un modello di embedding è una rete neurale che mappa testi in vettori numerici.\n",
    "2. Il training avviene imparando relazioni di co-occorrenza e predizione dal contesto.\n",
    "3. La similarità si misura con distanze matematiche nello spazio vettoriale.\n",
    "4. I modelli moderni risolvono l’ambiguità semantica generando embeddings **contestuali**, diversi a seconda della frase.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391c6b5-4a9f-4f31-8530-213a83f67b70",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 3.2 – Rate Limiting\n",
    "\n",
    "---\n",
    "\n",
    "## Che cos’è il rate limiting\n",
    "\n",
    "**Rate limiting** significa che ogni provider API impone un limite al numero di richieste che puoi fare in un dato intervallo di tempo.\n",
    "\n",
    "> Serve a **proteggere i server**, **limitare gli abusi**, **gestire il carico globale** e **controllare i costi** per ogni utente.\n",
    "\n",
    "### Esempio:\n",
    "\n",
    "* OpenAI può impostare un limite di **10 richieste al secondo per IP** oppure **60.000 token al minuto per account**.\n",
    "\n",
    "---\n",
    "\n",
    "## Perché esiste\n",
    "\n",
    "1. **Protezione dell’infrastruttura**\n",
    "\n",
    "   * Gli LLM sono risorse computazionalmente costose. Un uso incontrollato può sovraccaricare i server.\n",
    "   * Protegge contro DDoS (Distributed Denial of Service) e spam.\n",
    "\n",
    "2. **Gestione della qualità del servizio**\n",
    "\n",
    "   * Se ogni utente ha un limite, il servizio resta reattivo per tutti.\n",
    "   * Evita che un singolo utente monopolizzi l’intero throughput.\n",
    "\n",
    "3. **Controllo dei costi**\n",
    "\n",
    "   * Molti provider (es. Azure OpenAI, Hugging Face) fanno pagare a consumo.\n",
    "   * I limiti evitano spese impreviste o abusi accidentali da parte di sviluppatori inesperti.\n",
    "\n",
    "4. **Prevenzione di abusi e violazioni**\n",
    "\n",
    "   * Richieste automatizzate aggressive (bot) o scraping possono essere fermati.\n",
    "\n",
    "---\n",
    "\n",
    "## Tipi di limitazioni\n",
    "\n",
    "| Tipo di limite        | Esempio                                            |\n",
    "| --------------------- | -------------------------------------------------- |\n",
    "| Limite per secondo    | max 10 richieste/secondo                           |\n",
    "| Limite per minuto     | max 20.000 token/minuto                            |\n",
    "| Limite giornaliero    | max 1.000 chiamate/giorno                          |\n",
    "| Limite per modello    | GPT-4: 10k token/minuto, GPT-3.5: 60k token/minuto |\n",
    "| Limite per IP/API key | Ogni chiave ha il suo tetto                        |\n",
    "\n",
    "---\n",
    "\n",
    "## Codice di stato HTTP: `429 Too Many Requests`\n",
    "\n",
    "Se superi il rate limit, il server risponde con:\n",
    "\n",
    "```\n",
    "HTTP 429 TOO MANY REQUESTS\n",
    "```\n",
    "\n",
    "### Esempio di risposta:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"error\": {\n",
    "    \"message\": \"You exceeded your current quota. Please wait or reduce the rate of requests.\",\n",
    "    \"type\": \"rate_limit_exceeded\",\n",
    "    \"code\": \"429\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Come gestirlo nel codice\n",
    "\n",
    "### 1. **Retry automatico con backoff**\n",
    "\n",
    "Quando ricevi `429`, **non ritentare subito**. Usa una strategia di **backoff esponenziale** (aumenta il tempo di attesa a ogni tentativo).\n",
    "\n",
    "### Esempio con `tenacity` (Python)\n",
    "\n",
    "```python\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "import openai\n",
    "\n",
    "@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(5))\n",
    "def call_model():\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Cos'è il rate limiting?\"}]\n",
    "    )\n",
    "\n",
    "response = call_model()\n",
    "```\n",
    "\n",
    "* `wait_exponential`: attende 2s, poi 4s, poi 8s, ecc.\n",
    "* `stop_after_attempt(5)`: massimo 5 tentativi\n",
    "\n",
    "---\n",
    "\n",
    "## Come sapere qual è il proprio rate limit\n",
    "\n",
    "Ogni provider ha limiti documentati:\n",
    "\n",
    "* **OpenAI**: visibili nella dashboard → Usage → Rate Limits\n",
    "* **Azure OpenAI**: dipendono dal piano e dalla risorsa (es. S0 vs S1)\n",
    "* **Hugging Face**: limiti diversi tra Inference API pubblica e Endpoints dedicati\n",
    "\n",
    "Puoi anche ricevere info nei **response headers**:\n",
    "\n",
    "```http\n",
    "X-RateLimit-Limit: 100\n",
    "X-RateLimit-Remaining: 3\n",
    "X-RateLimit-Reset: 1667851234\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* Il rate limiting è una difesa essenziale contro abuso, costi imprevisti e degrado delle performance.\n",
    "* Il codice `429` indica che sei andato oltre i limiti.\n",
    "* Bisogna implementare retry automatici e gestire gli errori in modo robusto.\n",
    "* È buona pratica loggare tutte le risposte 429 e monitorare il proprio usage per evitare problemi in produzione.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9c1053-6db3-4c00-b7d7-20f3c06ec829",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 3.3 – Retry e Backoff\n",
    "\n",
    "---\n",
    "\n",
    "## Cos'è il \"retry\"\n",
    "\n",
    "Il **retry** (ripetizione) è una tecnica con cui un'applicazione ripete una richiesta API **dopo un errore temporaneo**, come un rate limit (`429`) o un timeout (`504`).\n",
    "\n",
    "> È fondamentale per garantire **robustezza** e **tolleranza ai fallimenti** in sistemi che usano modelli via API.\n",
    "\n",
    "---\n",
    "\n",
    "## Retry manuale vs automatico\n",
    "\n",
    "### Retry manuale\n",
    "\n",
    "Nel retry manuale:\n",
    "\n",
    "* Sei tu a scrivere la logica di controllo errori\n",
    "* Usi blocchi `try/except` e cicli `while` per decidere se e quando ritentare\n",
    "\n",
    "#### Esempio base:\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import time\n",
    "\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Spiegami il retry.\"}]\n",
    "        )\n",
    "        break  # Se funziona, esce dal ciclo\n",
    "    except openai.error.RateLimitError:\n",
    "        wait_time = 2 ** attempt  # backoff esponenziale\n",
    "        print(f\"Rate limit, attendo {wait_time}s...\")\n",
    "        time.sleep(wait_time)\n",
    "```\n",
    "\n",
    "### Retry automatico\n",
    "\n",
    "Nel retry automatico:\n",
    "\n",
    "* Una libreria gestisce tutto per te (ritenti, attese, log, limite massimo)\n",
    "* È configurabile, riutilizzabile e più pulita\n",
    "\n",
    "---\n",
    "\n",
    "## Strategie di backoff\n",
    "\n",
    "**Backoff** significa **attendere** prima di ritentare una richiesta fallita.\n",
    "\n",
    "### 1. Fisso\n",
    "\n",
    "Ogni retry attende lo stesso tempo: `sleep(2)` → `sleep(2)` → …\n",
    "\n",
    "### 2. Esponenziale (best practice)\n",
    "\n",
    "Ogni retry raddoppia il tempo d’attesa:\n",
    "`2s → 4s → 8s → 16s`\n",
    "\n",
    "### 3. Esponenziale con jitter (variazione casuale)\n",
    "\n",
    "Evita che più client facciano retry tutti insieme allo stesso istante.\n",
    "Molto usato nei sistemi distribuiti per evitare picchi sincronizzati.\n",
    "\n",
    "---\n",
    "\n",
    "## Uso della libreria `tenacity` (Python)\n",
    "\n",
    "[`tenacity`](https://tenacity.readthedocs.io/) è una libreria Python robusta e flessibile per retry automatici, con supporto per backoff, timeout, logging e controllo fine degli errori.\n",
    "\n",
    "### Installazione:\n",
    "\n",
    "```bash\n",
    "pip install tenacity\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Esempio completo con GPT-4\n",
    "\n",
    "```python\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),  # 2s, 4s, 8s, max 10s\n",
    "    stop=stop_after_attempt(5)  # massimo 5 tentativi\n",
    ")\n",
    "def ask():\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Cos'è il backoff esponenziale?\"}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=200\n",
    "    )\n",
    "\n",
    "response = ask()\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Parametri principali di `tenacity`\n",
    "\n",
    "| Parametro                   | Significato                                |\n",
    "| --------------------------- | ------------------------------------------ |\n",
    "| `wait_exponential()`        | Backoff esponenziale automatico            |\n",
    "| `stop_after_attempt(n)`     | Interrompe dopo n tentativi                |\n",
    "| `retry_if_exception_type()` | Specifica quali eccezioni far ritentare    |\n",
    "| `retry_error_callback()`    | Funzione da eseguire se i retry falliscono |\n",
    "| `before_sleep()`            | Funzione da chiamare prima di ogni attesa  |\n",
    "\n",
    "---\n",
    "\n",
    "## Quando usare il retry\n",
    "\n",
    "| Situazione                        | Retry? | Motivazione                         |\n",
    "| --------------------------------- | ------ | ----------------------------------- |\n",
    "| `429 Too Many Requests`           | yes      | Rate limit temporaneo               |\n",
    "| `503 Service Unavailable`         | yes      | Server temporaneamente sovraccarico |\n",
    "| `openai.error.APIConnectionError` | yes      | Problema di rete                    |\n",
    "| `400 Bad Request`                 | no      | Errore nel tuo codice o input       |\n",
    "| `401 Unauthorized`                | no      | Chiave API errata o scaduta         |\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* Il retry con backoff è essenziale per costruire **client resilienti** alle API LLM.\n",
    "* Usare librerie come `tenacity` ti permette di scrivere codice più pulito, riutilizzabile e conforme alle best practice cloud.\n",
    "* Implementarlo è indispensabile in qualsiasi **ambiente di produzione o batch processing**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f863d2-2dee-4fd2-ac6d-58d562e872ae",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Guida completa a `tenacity`\n",
    "\n",
    "**Retry intelligente con Python**\n",
    "\n",
    "---\n",
    "\n",
    "## Cos'è `tenacity`\n",
    "\n",
    "`tenacity` è una libreria Python progettata per **automatizzare e gestire il retry** di funzioni che possono fallire temporaneamente, come chiamate a:\n",
    "\n",
    "* API esterne (es. OpenAI, Azure, Hugging Face)\n",
    "* Sistemi distribuiti\n",
    "* Database o file remoti\n",
    "\n",
    "Permette di:\n",
    "\n",
    "* Ritentare automaticamente\n",
    "* Attendere tra un tentativo e l'altro\n",
    "* Limitare il numero massimo di retry\n",
    "* Loggare gli errori\n",
    "* Applicare backoff, jitter, e timeout\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Decoratore `@retry(...)`**\n",
    "\n",
    "È la base della libreria. Si applica a una funzione che vuoi rendere \"resiliente\".\n",
    "\n",
    "### Esempio minimo:\n",
    "\n",
    "```python\n",
    "from tenacity import retry\n",
    "\n",
    "@retry\n",
    "def instabile():\n",
    "    print(\"Tentativo in corso...\")\n",
    "    raise Exception(\"Errore temporaneo\")\n",
    "\n",
    "instabile()\n",
    "```\n",
    "\n",
    "Questo codice continua a ritentare **all’infinito** perché non abbiamo impostato limiti.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Controllare il numero di tentativi** – `stop_after_attempt()`\n",
    "\n",
    "Permette di interrompere dopo un certo numero di tentativi falliti.\n",
    "\n",
    "```python\n",
    "from tenacity import retry, stop_after_attempt\n",
    "\n",
    "@retry(stop=stop_after_attempt(3))\n",
    "def instabile():\n",
    "    print(\"Tentativo fallito\")\n",
    "    raise Exception(\"Errore\")\n",
    "\n",
    "instabile()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Tentativo fallito\n",
    "Tentativo fallito\n",
    "Tentativo fallito\n",
    "TenacityRetryError: RetryError ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Attendere tra i tentativi** – `wait_fixed()`, `wait_exponential()`, `wait_random()`\n",
    "\n",
    "### `wait_fixed(seconds)`\n",
    "\n",
    "Attende sempre lo stesso tempo.\n",
    "\n",
    "```python\n",
    "from tenacity import retry, wait_fixed\n",
    "\n",
    "@retry(wait=wait_fixed(2))\n",
    "def f():\n",
    "    print(\"Retry ogni 2 secondi\")\n",
    "    raise Exception()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `wait_exponential()`\n",
    "\n",
    "Attesa crescente: 2s, 4s, 8s, ...\n",
    "\n",
    "```python\n",
    "from tenacity import retry, wait_exponential\n",
    "\n",
    "@retry(wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "def f():\n",
    "    print(\"Backoff esponenziale\")\n",
    "    raise Exception()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `wait_random(min, max)`\n",
    "\n",
    "Ritardo casuale tra i due estremi.\n",
    "\n",
    "```python\n",
    "from tenacity import retry, wait_random\n",
    "\n",
    "@retry(wait=wait_random(min=1, max=5))\n",
    "def f():\n",
    "    print(\"Attesa casuale tra 1 e 5 secondi\")\n",
    "    raise Exception()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Condizioni per il retry** – `retry_if_exception_type()`, `retry_if_result()`\n",
    "\n",
    "### `retry_if_exception_type(MyException)`\n",
    "\n",
    "Ritentare **solo per eccezioni specifiche**.\n",
    "\n",
    "```python\n",
    "from tenacity import retry, retry_if_exception_type\n",
    "\n",
    "class Retryable(Exception): pass\n",
    "class NonRetryable(Exception): pass\n",
    "\n",
    "@retry(retry=retry_if_exception_type(Retryable))\n",
    "def f():\n",
    "    raise Retryable()  # se cambi in NonRetryable, non ritenta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `retry_if_result(...)`\n",
    "\n",
    "Ritentare **in base al risultato** restituito dalla funzione (non solo se fallisce).\n",
    "\n",
    "```python\n",
    "from tenacity import retry, retry_if_result\n",
    "\n",
    "@retry(retry=retry_if_result(lambda r: r is None))\n",
    "def f():\n",
    "    print(\"Funzione ha restituito None, ritento...\")\n",
    "    return None\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Callback di logging/debug** – `before`, `after`, `before_sleep`, `retry_error_callback`\n",
    "\n",
    "### `before_sleep`\n",
    "\n",
    "Funzione da chiamare **prima del prossimo retry**.\n",
    "\n",
    "```python\n",
    "from tenacity import retry, wait_fixed, before_sleep\n",
    "\n",
    "def avviso(retry_state):\n",
    "    print(f\"Errore: nuovo tentativo in {retry_state.next_action.sleep} secondi...\")\n",
    "\n",
    "@retry(wait=wait_fixed(2), before_sleep=avviso, stop=stop_after_attempt(3))\n",
    "def f():\n",
    "    raise Exception(\"Errore\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `retry_error_callback`\n",
    "\n",
    "Funzione chiamata **dopo che tutti i retry falliscono**.\n",
    "\n",
    "```python\n",
    "from tenacity import retry, stop_after_attempt, retry_error_callback\n",
    "\n",
    "def fallback(retry_state):\n",
    "    return \"Default response\"\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), retry_error_callback=fallback)\n",
    "def f():\n",
    "    raise Exception(\"Errore grave\")\n",
    "\n",
    "print(f())  # -> \"Default response\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Combinazioni complesse** – `retry_any`, `retry_all`\n",
    "\n",
    "Per condizioni multiple.\n",
    "\n",
    "```python\n",
    "from tenacity import retry, retry_any, retry_if_exception_type, retry_if_result\n",
    "\n",
    "@retry(retry=retry_any(\n",
    "    retry_if_exception_type(ValueError),\n",
    "    retry_if_result(lambda r: r is None)\n",
    "))\n",
    "def f():\n",
    "    raise ValueError()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Logging automatico integrato**\n",
    "\n",
    "Tenacity può integrarsi con la libreria `logging` di Python. Esempio:\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(1), reraise=True)\n",
    "def f():\n",
    "    raise Exception(\"Errore\")\n",
    "\n",
    "f()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "| Funzione                    | Cosa fa                                     |\n",
    "| --------------------------- | ------------------------------------------- |\n",
    "| `@retry`                    | Attiva la logica di retry                   |\n",
    "| `stop_after_attempt(n)`     | Limita i tentativi                          |\n",
    "| `wait_fixed(t)`             | Pausa fissa tra i retry                     |\n",
    "| `wait_exponential()`        | Pausa crescente (backoff esponenziale)      |\n",
    "| `retry_if_exception_type()` | Retry solo su specifiche eccezioni          |\n",
    "| `retry_if_result()`         | Retry se il risultato è nullo o sbagliato   |\n",
    "| `before_sleep()`            | Log personalizzato prima del prossimo retry |\n",
    "| `retry_error_callback()`    | Funzione fallback dopo i fallimenti         |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07a079-3550-4a8c-9ba4-fdc379f3395e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 4.1 – Batch Completions\n",
    "\n",
    "---\n",
    "\n",
    "## Cos'è una \"batch completion\"\n",
    "\n",
    "Una **batch completion** consiste nell'inviare **più richieste di completamento (o embedding)** in un **singolo blocco** a un endpoint API, invece di eseguire una chiamata alla volta.\n",
    "\n",
    "> L’obiettivo è aumentare l’efficienza elaborando più richieste in parallelo o in serie ottimizzata.\n",
    "\n",
    "È utile in tutti i casi in cui:\n",
    "\n",
    "* Devi generare molti testi\n",
    "* Devi calcolare molti embeddings\n",
    "* Lavori con dataset di grandi dimensioni\n",
    "\n",
    "---\n",
    "\n",
    "## Come si realizza una batch completion\n",
    "\n",
    "Dipende dal **tipo di endpoint**.\n",
    "\n",
    "### 1. Batch con `embeddings` (supportato direttamente)\n",
    "\n",
    "Molti provider accettano **una lista di stringhe** come input:\n",
    "\n",
    "```python\n",
    "response = openai.Embedding.create(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    input=[\n",
    "        \"Eventi in montagna\",\n",
    "        \"Guida alle escursioni\",\n",
    "        \"Campeggi invernali\"\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "Ricevi in risposta un **vettore per ogni frase**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Batch con `completions` o `chat` (non supportato direttamente)\n",
    "\n",
    "OpenAI e Azure non permettono nativamente chiamate chat/multiple in batch con un solo payload, quindi la soluzione è:\n",
    "\n",
    "* **Parallelizzare** lato client (threading, asyncio, multiprocessing)\n",
    "* **Ottimizzare** lato codice con retry e gestione delle chiamate simultanee\n",
    "\n",
    "---\n",
    "\n",
    "## Vantaggi del batch processing\n",
    "\n",
    "| Vantaggio                    | Descrizione                                                       |\n",
    "| ---------------------------- | ----------------------------------------------------------------- |\n",
    "| Throughput più elevato       | È più efficiente fare 10 chiamate in parallelo che 10 in sequenza |\n",
    "| Utilizzo ottimale della rete | Si riduce l’overhead per singola chiamata                         |\n",
    "| Utilizzo di GPU server-side  | I modelli LLM processano meglio lotti più grandi                  |\n",
    "\n",
    "---\n",
    "\n",
    "## Limiti e svantaggi\n",
    "\n",
    "| Limite                               | Dettagli                                                                                                |\n",
    "| ------------------------------------ | ------------------------------------------------------------------------------------------------------- |\n",
    "| **Latenza cumulativa**               | Se una chiamata batch contiene 100 input, bisogna aspettare che **tutti** siano completati              |\n",
    "| **Gestione errori più complessa**    | Se uno degli input fallisce, può essere necessario ripetere l’intero batch o escludere quello specifico |\n",
    "| **Quota token condivisa**            | Il limite di token per batch è comunque **totale** (es. 8.000 token input/output sommati)               |\n",
    "| **Maggiore complessità nel logging** | Bisogna tracciare ogni risposta all’interno del batch manualmente                                       |\n",
    "\n",
    "---\n",
    "\n",
    "## Esempio pratico con `concurrent.futures.ThreadPoolExecutor` (OpenAI chat)\n",
    "\n",
    "### Obiettivo: inviare 5 prompt diversi in parallelo\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "prompts = [\n",
    "    \"Scrivi un'introduzione su eventi in montagna\",\n",
    "    \"Genera una descrizione per un festival di yoga\",\n",
    "    \"Spiega il campeggio libero in Italia\",\n",
    "    \"Crea una lista di 5 eventi outdoor per famiglie\",\n",
    "    \"Descrivi un'escursione invernale in Trentino\"\n",
    "]\n",
    "\n",
    "def ask(prompt):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(ask, p) for p in prompts]\n",
    "    for future in as_completed(futures):\n",
    "        print(future.result())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Quando usare le batch completions\n",
    "\n",
    "| Scenario                            | Batch consigliato?          |\n",
    "| ----------------------------------- | --------------------------- |\n",
    "| Calcolo di molti embeddings         |  Sì                        |\n",
    "| Generazione di testi per un dataset |  Sì, via threading o async |\n",
    "| Conversazioni singole e interattive |  No                        |\n",
    "| UI con risposta in tempo reale      |  No                        |\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* Le **batch completions** aumentano il throughput e riducono i costi operativi su molti task simili.\n",
    "* Sono ideali per operazioni **su grandi volumi**, come generazione automatica, embedding, classificazione, sintesi.\n",
    "* Serve gestire bene i **limiti di token**, le **eccezioni individuali** e implementare **log granularity**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e51e45-5cbc-4d40-a0c0-86e9432ae87e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Cos'è `asyncio` in Python\n",
    "\n",
    "---\n",
    "\n",
    "##  Definizione\n",
    "\n",
    "`asyncio` è il **framework nativo di Python per la programmazione asincrona**.\n",
    "Serve per **eseguire più operazioni contemporaneamente** (in modo cooperativo), **senza bloccare il thread principale**.\n",
    "\n",
    "> Non usa thread multipli o processi, ma gestisce le attese (I/O, rete, attesa API) in modo efficiente e non bloccante.\n",
    "\n",
    "---\n",
    "\n",
    "##  Quando serve\n",
    "\n",
    "È utile quando:\n",
    "\n",
    "* Hai **molte operazioni I/O-bound**, come chiamate API o accessi a file o DB.\n",
    "* Vuoi **parallelizzare** codice senza usare thread o processi.\n",
    "* Vuoi **risposte rapide da molte fonti** (batch API, scraping, chiamate concorrenti).\n",
    "\n",
    "---\n",
    "\n",
    "##  Quando **non** serve\n",
    "\n",
    "* Per task **CPU-bound** (es. calcoli pesanti): meglio usare `multiprocessing`.\n",
    "* Se hai solo una richiesta alla volta: l’overhead non vale la pena.\n",
    "\n",
    "---\n",
    "\n",
    "# Concetti base\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `async def`\n",
    "\n",
    "È una funzione asincrona: puoi sospenderla e riprenderla quando vuoi.\n",
    "\n",
    "```python\n",
    "async def saluta():\n",
    "    print(\"Ciao\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `await`\n",
    "\n",
    "Serve per **sospendere l’esecuzione** fino a quando una funzione asincrona non ha finito.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def saluta():\n",
    "    print(\"Ciao\")\n",
    "    await asyncio.sleep(1)  # pausa non bloccante\n",
    "    print(\"Benvenuto!\")\n",
    "\n",
    "asyncio.run(saluta())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `asyncio.gather`\n",
    "\n",
    "Serve per **eseguire più funzioni asincrone in parallelo** e aspettare che tutte finiscano.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def attesa(x):\n",
    "    await asyncio.sleep(x)\n",
    "    print(f\"Dormito {x} secondi\")\n",
    "\n",
    "async def main():\n",
    "    await asyncio.gather(attesa(1), attesa(2), attesa(3))\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Dopo 1s → \"Dormito 1 secondi\"\n",
    "Dopo 2s → \"Dormito 2 secondi\"\n",
    "Dopo 3s → \"Dormito 3 secondi\"\n",
    "```\n",
    "\n",
    "Tempo totale: 3s, non 6s.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Esempio con API: chiamate asincrone OpenAI\n",
    "\n",
    "La libreria `openai` ufficiale **non è asincrona**, ma si può adattare.\n",
    "Tuttavia, per API REST generiche (es. con `httpx`, `aiohttp`) è molto comodo.\n",
    "\n",
    "### Esempio con `httpx` (client asincrono)\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "import httpx\n",
    "import os\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "URL = \"https://api.openai.com/v1/completions\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "async def fetch_completion(prompt):\n",
    "    payload = {\n",
    "        \"model\": \"text-davinci-003\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.post(URL, headers=HEADERS, json=payload)\n",
    "        data = response.json()\n",
    "        return data[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "async def main():\n",
    "    prompts = [\n",
    "        \"Scrivi una poesia sulla neve\",\n",
    "        \"Descrivi un’escursione in montagna\",\n",
    "        \"Spiega il concetto di backoff esponenziale\",\n",
    "    ]\n",
    "\n",
    "    tasks = [fetch_completion(p) for p in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Vantaggi pratici di `asyncio` per chiamate API\n",
    "\n",
    "| Vantaggio              | Descrizione                                               |\n",
    "| ---------------------- | --------------------------------------------------------- |\n",
    "| Maggiore efficienza    | Una singola CPU può gestire centinaia di chiamate I/O     |\n",
    "| Minore latenza totale  | Le chiamate partono insieme, si attende solo la più lenta |\n",
    "| Uso ridotto di risorse | Non servono thread multipli né processi                   |\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* `asyncio` è lo standard Python per la **programmazione asincrona non bloccante**.\n",
    "* Ti permette di **parallelizzare operazioni lente (I/O)** come chiamate API.\n",
    "* Usato con `httpx`, `aiohttp`, o librerie compatibili con async, ti permette di **costruire sistemi scalabili** con poche righe di codice.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017391a-61e8-4eed-bf33-79db32384d71",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 4.2 – Streaming completions\n",
    "\n",
    "---\n",
    "\n",
    "## Cos’è lo streaming (`stream=True`)\n",
    "\n",
    "Nel contesto delle API LLM (come OpenAI o Azure OpenAI), abilitare lo **streaming** significa ricevere l’output **man mano che il modello lo genera**, **token per token**, invece che attendere che l'intera risposta sia pronta.\n",
    "\n",
    "Si attiva passando il parametro:\n",
    "\n",
    "```json\n",
    "\"stream\": true\n",
    "```\n",
    "\n",
    "Durante lo streaming, il server invia una **serie di eventi** incrementali (tipicamente con tecnologia Server-Sent Events o WebSocket), ognuno contenente una piccola parte del completamento, fino al termine della generazione.\n",
    "\n",
    "---\n",
    "\n",
    "### Differenza tra modalità normale e streaming\n",
    "\n",
    "| Modalità           | Output                                 | Esperienza                                             |\n",
    "| ------------------ | -------------------------------------- | ------------------------------------------------------ |\n",
    "| Normale (default)  | Tutta la risposta in un’unica volta    | Bloccante: l’utente aspetta finché non è tutto pronto  |\n",
    "| Streaming (`true`) | Frammenti incrementali (token o frasi) | Reattiva: l’utente vede il testo mentre viene generato |\n",
    "\n",
    "---\n",
    "\n",
    "## Come gestirlo lato backend\n",
    "\n",
    "### Utilizzo con OpenAI SDK (Python)\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Spiegami lo streaming completions\"}\n",
    "    ],\n",
    "    stream=True  # attiva lo streaming\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    delta = chunk[\"choices\"][0][\"delta\"]\n",
    "    if \"content\" in delta:\n",
    "        print(delta[\"content\"], end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "### Struttura del flusso\n",
    "\n",
    "Ogni `chunk` ricevuto contiene solo il **delta**, cioè la parte appena generata. Alla fine, il modello invia un messaggio con `finish_reason`.\n",
    "\n",
    "Per esempio:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"delta\": {\n",
    "        \"content\": \"Questo è un \"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Successivamente:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"delta\": {\n",
    "        \"content\": \"esempio di streaming\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "E così via, fino al completamento.\n",
    "\n",
    "---\n",
    "\n",
    "## Come gestirlo lato frontend\n",
    "\n",
    "Per creare interfacce moderne e reattive (es. chatbot), è necessario che anche il frontend sia in grado di **ricevere ed esporre il contenuto parziale in tempo reale**.\n",
    "\n",
    "### Architettura tipica\n",
    "\n",
    "1. Il client invia un prompt al backend\n",
    "2. Il backend apre una chiamata con `stream=True` al modello\n",
    "3. Il backend trasmette via **WebSocket** o **EventSource** ogni frammento al client\n",
    "4. Il frontend aggiorna la UI **progressivamente**\n",
    "\n",
    "### Tecnologie usate\n",
    "\n",
    "* **Backend**: FastAPI, Flask, Node.js, Django Channels\n",
    "* **Streaming**: WebSocket, Server-Sent Events (SSE)\n",
    "* **Frontend**: React, Vue, Svelte, ecc. con listener asincroni\n",
    "\n",
    "---\n",
    "\n",
    "## Quando usare lo streaming\n",
    "\n",
    "Lo streaming è fortemente consigliato quando:\n",
    "\n",
    "| Contesto                            | Motivazione                                                      |\n",
    "| ----------------------------------- | ---------------------------------------------------------------- |\n",
    "| Chatbot avanzati                    | Migliora la percezione di reattività e fluidità                  |\n",
    "| Applicazioni interattive            | Riduce il tempo di attesa percepito                              |\n",
    "| Ambienti vocali / assistenti vocali | Consente di iniziare a leggere o parlare prima del completamento |\n",
    "| Generazione creativa                | Utile per ispirare o interrompere la generazione a metà          |\n",
    "| Sistemi di feedback in tempo reale  | Permette di fermare l'inferenza se non soddisfa i criteri        |\n",
    "\n",
    "---\n",
    "\n",
    "## Vantaggi dello streaming\n",
    "\n",
    "* Migliore esperienza utente (l’output appare subito)\n",
    "* Maggiore controllo: puoi interrompere il flusso a metà\n",
    "* Migliore integrazione con UI dinamiche e conversazioni complesse\n",
    "* Riduzione della percezione della latenza\n",
    "\n",
    "---\n",
    "\n",
    "## Limitazioni\n",
    "\n",
    "* Richiede architettura asincrona lato backend\n",
    "* Più complesso da implementare rispetto a una semplice `POST`\n",
    "* Non disponibile su tutti i provider o modelli\n",
    "* Maggior complessità nella gestione dell'errore parziale e dell’interruzione\n",
    "\n",
    "---\n",
    "\n",
    "## In sintesi\n",
    "\n",
    "* Con lo **streaming completions**, non si aspetta più l'intera risposta, ma si riceve il testo progressivamente.\n",
    "* Questo consente esperienze molto più fluide e naturali, soprattutto in chatbot e agenti conversazionali.\n",
    "* Richiede attenzione all'implementazione, ma è uno standard per ogni sistema LLM moderno orientato all’interazione.\n",
    "\n",
    "---\n",
    "\n",
    "Fammi sapere se vuoi:\n",
    "\n",
    "* Un esempio completo `FastAPI` + WebSocket per gestire lo streaming lato backend e frontend\n",
    "* Oppure proseguire con la **Parte 5 – Azure OpenAI: tiers, regioni, deployment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815a0f2c-49ac-4f9b-a492-336f5a167e23",
   "metadata": {},
   "source": [
    "Ecco una **versione semplificata** con **Streamlit** che invia un **payload JSON** (solo `prompt`) a **LM Studio** e mostra la risposta **in streaming**, integrando **retry** ed **async** in modo pulito.\n",
    "\n",
    "---\n",
    "\n",
    "### requirements.txt\n",
    "\n",
    "```txt\n",
    "streamlit==1.48.0\n",
    "httpx==0.27.0\n",
    "tenacity==9.1.2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### app.py\n",
    "\n",
    "```python\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import streamlit as st\n",
    "import httpx\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# Endpoint OpenAI-compatible di LM Studio (Local Server)\n",
    "LMSTUDIO_URL = os.getenv(\"LMSTUDIO_URL\", \"http://localhost:1234/v1/chat/completions\")\n",
    "\n",
    "st.set_page_config(page_title=\"LM Studio Stream Demo\", layout=\"centered\")\n",
    "st.title(\"LM Studio – Streaming via API (Flask-free, Streamlit-only)\")\n",
    "\n",
    "st.markdown(\n",
    "    \"\"\"\n",
    "**Istruzioni rapide**\n",
    "1. Apri LM Studio, carica un modello e attiva il *Local Server* (OpenAI-compatible) sulla porta `1234`.\n",
    "2. Inserisci un prompt qui sotto e premi **Genera**.\n",
    "3. Vedrai l’output arrivare **in streaming**.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Parametri UI\n",
    "prompt = st.text_area(\"Prompt\", height=150, placeholder=\"Scrivi qui il prompt...\")\n",
    "c1, c2, c3 = st.columns(3)\n",
    "with c1:\n",
    "    temperature = st.slider(\"Temperature\", 0.0, 1.5, 0.2, 0.1)\n",
    "with c2:\n",
    "    max_tokens = st.number_input(\"Max tokens\", min_value=16, max_value=4096, value=512, step=16)\n",
    "with c3:\n",
    "    model = st.text_input(\"Model (opzionale)\", value=\"local-llm\")\n",
    "\n",
    "go = st.button(\"Genera\", type=\"primary\", use_container_width=True)\n",
    "\n",
    "HEADERS = {\"Content-Type\": \"application/json\",\n",
    "          \"Accept\": \"text/event-stream\"  # per lo stream SSE\n",
    "          }\n",
    "\n",
    "# --- FUNZIONE ASINCRONA CON RETRY E BACKOFF -------------------------------\n",
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=8),\n",
    "    stop=stop_after_attempt(4),\n",
    "    retry=retry_if_exception_type((\n",
    "        httpx.ConnectError,\n",
    "        httpx.ReadError,\n",
    "        httpx.RemoteProtocolError,\n",
    "        httpx.TimeoutException,\n",
    "        httpx.HTTPStatusError,\n",
    "    )),\n",
    "    reraise=True,\n",
    ")\n",
    "async def stream_lmstudio(prompt_text: str, temperature: float, max_tokens: int, model_name: str):\n",
    "    \"\"\"\n",
    "    Effettua una chiamata al server OpenAI-compatible di LM Studio con stream=True\n",
    "    e restituisce un async generator di 'pezzi' di testo man mano che arrivano.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_name or \"local-llm\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": True,  # chiave per lo streaming token-by-token\n",
    "    }\n",
    "\n",
    "    timeout = httpx.Timeout(connect=10.0, read=600.0, write=30.0, pool=10.0)\n",
    "    async with httpx.AsyncClient(timeout=timeout) as client:\n",
    "        async with client.stream(\"POST\", LMSTUDIO_URL, headers=HEADERS, json=payload) as resp:\n",
    "            # Genera HTTPStatusError se 4xx/5xx (così scatta il retry)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            # LM Studio emette SSE \"data: {...}\" + \"[DONE]\"\n",
    "            async for line in resp.aiter_lines():\n",
    "                if not line:\n",
    "                    continue\n",
    "                if not line.startswith(\"data: \"):\n",
    "                    # Alcuni server potrebbero inviare righe di keep-alive/commenti\n",
    "                    continue\n",
    "\n",
    "                data_line = line[6:].strip()\n",
    "                if data_line == \"[DONE]\":\n",
    "                    break\n",
    "\n",
    "                # Prova a decodificare JSON nel formato OpenAI \"delta\"\n",
    "                piece = None\n",
    "                try:\n",
    "                    obj = json.loads(data_line)\n",
    "                    delta = obj[\"choices\"][0].get(\"delta\", {})\n",
    "                    piece = delta.get(\"content\", \"\")\n",
    "                except Exception:\n",
    "                    # Se non è JSON valido, emetti comunque la riga grezza\n",
    "                    piece = data_line\n",
    "\n",
    "                if piece:\n",
    "                    yield piece\n",
    "\n",
    "# --- CONSUMER ASINCRONO -> AGGIORNA LA UI --------------------------------\n",
    "\n",
    "async def consume_stream_and_render(prompt_text: str, temperature: float, max_tokens: int, model_name: str):\n",
    "    placeholder = st.empty()\n",
    "    buffer = []\n",
    "\n",
    "    try:\n",
    "        async for chunk in stream_lmstudio(prompt_text, temperature, max_tokens, model_name):\n",
    "            buffer.append(chunk)\n",
    "            # Aggiornamento progressivo (Markdown per andare a capo correttamente)\n",
    "            placeholder.markdown(\"\".join(buffer))\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        # Errori 4xx/5xx non recuperabili dopo i retry\n",
    "        st.error(f\"Errore dal server LM Studio: {e.response.status_code} - {e.response.text}\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Errore di rete o streaming: {e}\")\n",
    "\n",
    "# --- HANDLER BOTTONE ------------------------------------------------------\n",
    "\n",
    "if go:\n",
    "    if not prompt.strip():\n",
    "        st.warning(\"Inserisci un prompt prima di generare.\")\n",
    "    else:\n",
    "        st.info(\"Connessione a LM Studio e streaming in corso…\")\n",
    "        # Esegue il consumer asincrono (Streamlit è sync, quindi lanciamo l'event loop qui)\n",
    "        asyncio.run(consume_stream_and_render(prompt, temperature, max_tokens, model))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Come avviare\n",
    "\n",
    "1. Avvia **LM Studio**, carica un modello e attiva il **Local Server** (OpenAI-compatible) su `http://localhost:1234`.\n",
    "2. Installa i pacchetti:\n",
    "\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "3. Avvia Streamlit:\n",
    "\n",
    "   ```bash\n",
    "   streamlit run app.py\n",
    "   ```\n",
    "4. Apri il browser all’URL che Streamlit stampa (di solito `http://localhost:8501`).\n",
    "\n",
    "---\n",
    "\n",
    "## Note tecniche\n",
    "\n",
    "* **Streaming**: gestito via `httpx.AsyncClient.stream(...).aiter_lines()` parsando le righe SSE (`data: ...`). Riconosce `[DONE]`.\n",
    "* **Retry & Backoff**: `tenacity` ritenta automaticamente su errori di rete o `HTTPStatusError`, con backoff esponenziale. Non ritenta all’infinito e rilancia l’eccezione dopo 4 tentativi.\n",
    "* **Async**: il flusso è completamente asincrono; Streamlit (sincrono) lancia l’event loop con `asyncio.run(...)` al click.\n",
    "* **Sicurezza**: nessuna API key richiesta perché LM Studio è locale; se servisse, aggiungi un header `Authorization` lato LM Studio o proteggi la porta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6da89-89dc-417b-a048-b4f2ea17fa98",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Cos'è Streamlit\n",
    "\n",
    "---\n",
    "\n",
    "##  Definizione\n",
    "\n",
    "**Streamlit** è un framework Python open source pensato per creare **applicazioni web interattive e veloci** direttamente dal codice Python, **senza dover scrivere HTML, CSS o JavaScript**.\n",
    "\n",
    "È particolarmente popolare tra:\n",
    "\n",
    "* Data scientist\n",
    "* Sviluppatori AI/ML\n",
    "* Ricercatori e professionisti tecnici\n",
    "\n",
    "> Lo scopo è passare da uno script Python a un’interfaccia web funzionale **in pochi minuti**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Cosa permette di fare\n",
    "\n",
    "Con Streamlit puoi:\n",
    "\n",
    "* Mostrare grafici, tabelle, dataframe\n",
    "* Visualizzare testo formattato e codice\n",
    "* Creare **form di input dinamici** (testi, slider, select box)\n",
    "* Visualizzare in tempo reale risultati di modelli ML\n",
    "* Integrare modelli LLM, API e processi asincroni\n",
    "* **Condividere** il tuo progetto via web (es. su [streamlit.io](https://share.streamlit.io))\n",
    "\n",
    "---\n",
    "\n",
    "##  Come funziona\n",
    "\n",
    "Streamlit esegue lo script Python come un **flusso dichiarativo**:\n",
    "\n",
    "1. Tu scrivi codice Python come se fosse uno script di analisi o interazione.\n",
    "2. Ogni volta che l’utente interagisce con un elemento (es. clic, selezione, testo), **l’intero script viene rieseguito** automaticamente.\n",
    "3. Lo stato dell’app viene gestito in modo trasparente tramite variabili o `st.session_state`.\n",
    "\n",
    "---\n",
    "\n",
    "##  Avvio rapido\n",
    "\n",
    "### 1. Installazione\n",
    "\n",
    "```bash\n",
    "pip install streamlit\n",
    "```\n",
    "\n",
    "### 2. Script base (`app.py`)\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Ciao dal tuo primo Streamlit app\")\n",
    "name = st.text_input(\"Come ti chiami?\")\n",
    "if name:\n",
    "    st.success(f\"Benvenuto, {name}!\")\n",
    "```\n",
    "\n",
    "### 3. Avvio\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "Si apre il browser in automatico (es. `http://localhost:8501`) con l’interfaccia.\n",
    "\n",
    "---\n",
    "\n",
    "##  Esempi di componenti interattivi\n",
    "\n",
    "| Funzione           | Descrizione                          |\n",
    "| ------------------ | ------------------------------------ |\n",
    "| `st.text_input()`  | Campo di testo per l’utente          |\n",
    "| `st.slider()`      | Slider numerico                      |\n",
    "| `st.button()`      | Bottone cliccabile                   |\n",
    "| `st.selectbox()`   | Menu a tendina                       |\n",
    "| `st.text_area()`   | Area di testo multi-linea            |\n",
    "| `st.markdown()`    | Visualizzazione di testo formattato  |\n",
    "| `st.empty()`       | Placeholder dinamico                 |\n",
    "| `st.columns()`     | Layout in colonne                    |\n",
    "| `st.session_state` | Memorizzazione persistente tra rerun |\n",
    "\n",
    "---\n",
    "\n",
    "##  Backend e frontend in uno\n",
    "\n",
    "Streamlit **nasconde tutta la complessità del frontend**:\n",
    "\n",
    "* Tu scrivi solo Python\n",
    "* I componenti sono automaticamente sincronizzati tra Python (backend) e l’interfaccia utente (frontend)\n",
    "* Tutto gira su un server locale (o cloud) senza dover gestire Flask, React, WebSocket ecc.\n",
    "\n",
    "---\n",
    "\n",
    "##  Quando usare Streamlit\n",
    "\n",
    "### È ideale per:\n",
    "\n",
    "* Prototipi rapidi di interfacce AI\n",
    "* Dashboard di analisi dati\n",
    "* UI leggere per modelli NLP e computer vision\n",
    "* Demo da condividere con altri\n",
    "\n",
    "### Meno adatto a:\n",
    "\n",
    "* Web app complesse e multi-pagina (meglio Django o FastAPI + frontend)\n",
    "* Sistemi di autenticazione avanzati (richiede estensioni)\n",
    "* Uso ad alta concorrenza (non pensato per produzione enterprise pesante)\n",
    "\n",
    "---\n",
    "\n",
    "##  Internamente: come gestisce l’interazione\n",
    "\n",
    "Ogni volta che interagisci con un widget:\n",
    "\n",
    "1. L’input viene inviato al server Python\n",
    "2. Streamlit **riesegue lo script da cima a fondo**\n",
    "3. Gli elementi `st.text_input()`, `st.button()` ecc. restituiscono nuovi valori\n",
    "4. Tu usi if/else e `st.session_state` per determinare cosa aggiornare\n",
    "\n",
    "---\n",
    "\n",
    "##  In sintesi\n",
    "\n",
    "* **Cos'è**: un framework Python per creare web app interattive senza frontend.\n",
    "* **Come funziona**: dichiarativo e reattivo, riesegue lo script a ogni input.\n",
    "* **Vantaggi**: velocissimo da usare, ideale per AI, prototipi e visualizzazioni.\n",
    "* **Perfetto per**: app personali o interne che usano modelli, dati o API.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479fddc1-5399-4e40-b27f-af4f0f8642c9",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. **Configurazione iniziale della pagina**\n",
    "\n",
    "```python\n",
    "st.set_page_config(page_title=\"LM Studio Stream Demo\", layout=\"centered\")\n",
    "st.title(\"LM Studio – Streaming via API (Streamlit)\")\n",
    "st.markdown(\"\"\"...\"\"\")\n",
    "```\n",
    "\n",
    "###  Cosa fa\n",
    "\n",
    "* Imposta il titolo del browser e il layout della pagina.\n",
    "* Inserisce un titolo (`st.title`) e delle istruzioni testuali (`st.markdown`) nella UI.\n",
    "\n",
    "###  Obiettivo\n",
    "\n",
    "Spiegare all’utente come avviare LM Studio, inserire il prompt e ottenere l’output generato dal modello.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Costruzione dell’interfaccia di input**\n",
    "\n",
    "```python\n",
    "prompt = st.text_area(\"Prompt\", ...)\n",
    "c1, c2, c3 = st.columns(3)\n",
    "temperature = st.slider(...)\n",
    "max_tokens = st.number_input(...)\n",
    "model = st.text_input(...)\n",
    "go = st.button(\"Genera\", ...)\n",
    "```\n",
    "\n",
    "###  Cosa fa\n",
    "\n",
    "* `st.text_area`: area di input testuale multi-linea per inserire il prompt.\n",
    "* `st.columns`: crea una disposizione in 3 colonne per ordinare i controlli.\n",
    "* `st.slider`, `st.number_input`, `st.text_input`: raccolgono parametri aggiuntivi del modello (temperature, max\\_tokens, nome modello).\n",
    "* `st.button`: bottone per avviare la generazione.\n",
    "\n",
    "###  Obiettivo\n",
    "\n",
    "Permettere all’utente di personalizzare ogni richiesta in modo semplice e visivo, senza scrivere codice.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Visualizzazione dinamica dello streaming**\n",
    "\n",
    "```python\n",
    "async def consume_stream_and_render(...):\n",
    "    placeholder = st.empty()\n",
    "    buffer = []\n",
    "    ...\n",
    "    placeholder.markdown(\"\".join(buffer))\n",
    "```\n",
    "\n",
    "###  Cosa fa\n",
    "\n",
    "* `st.empty()` crea un **segnaposto vuoto** nella pagina.\n",
    "* Questo placeholder viene aggiornato progressivamente con i chunk testuali ricevuti dal modello.\n",
    "* Il buffer viene popolato ad ogni `chunk`, e il contenuto viene aggiornato in tempo reale nella UI.\n",
    "\n",
    "###  Obiettivo\n",
    "\n",
    "Simulare la sensazione di **output in streaming**, come un chatbot o un assistente che risponde mentre scrive.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Gestione asincrona e compatibilità con Streamlit**\n",
    "\n",
    "```python\n",
    "def run_async(coro):\n",
    "    ...\n",
    "```\n",
    "\n",
    "###  Cosa fa\n",
    "\n",
    "Streamlit non gestisce nativamente funzioni `async`. Questo blocco:\n",
    "\n",
    "* Verifica se c’è già un event loop attivo (cosa comune nei runtime Web).\n",
    "* Se esiste, lancia la coroutine tramite `run_coroutine_threadsafe`.\n",
    "* Altrimenti la esegue direttamente con `asyncio.run`.\n",
    "\n",
    "###  Obiettivo\n",
    "\n",
    "Consentire la **compatibilità asincrona** con lo streaming (`async for`) all’interno di un’app Streamlit che normalmente è sincrona.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Gestione degli eventi utente (clic su \"Genera\")**\n",
    "\n",
    "```python\n",
    "if go:\n",
    "    if not prompt.strip():\n",
    "        st.warning(...)\n",
    "    else:\n",
    "        st.info(\"Connessione a LM Studio e streaming in corso…\")\n",
    "        try:\n",
    "            run_async(consume_stream_and_render(...))\n",
    "        ...\n",
    "```\n",
    "\n",
    "###  Cosa fa\n",
    "\n",
    "* Quando l’utente preme il bottone `Genera`, questo blocco esegue il codice.\n",
    "* Verifica che il prompt non sia vuoto.\n",
    "* Avvia lo streaming asincrono del modello.\n",
    "* Gestisce eventuali errori di rete o di chiamata all’API mostrando messaggi (`st.warning`, `st.error`).\n",
    "\n",
    "###  Obiettivo\n",
    "\n",
    "Far partire il flusso di generazione **solo dopo interazione umana**, e fornire **feedback visivo** immediato su errori o stato del sistema.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Output in tempo reale dell’LLM**\n",
    "\n",
    "L’output è costruito e aggiornato in questa sezione:\n",
    "\n",
    "```python\n",
    "async for chunk in stream_lmstudio(...):\n",
    "    buffer.append(chunk)\n",
    "    placeholder.markdown(\"\".join(buffer))\n",
    "```\n",
    "\n",
    "###  Cosa fa\n",
    "\n",
    "* Usa uno stream HTTP con `httpx` e `stream=True`\n",
    "* Ogni \"pezzo\" del testo ricevuto viene subito mostrato all’utente nella UI\n",
    "* L’utente vede l’output crescere man mano\n",
    "\n",
    "---\n",
    "\n",
    "##  In sintesi\n",
    "\n",
    "| Funzione Streamlit             | Uso nello script                             |\n",
    "| ------------------------------ | -------------------------------------------- |\n",
    "| `st.set_page_config`           | Configurazione della pagina                  |\n",
    "| `st.title`, `st.markdown`      | Intestazioni e testo                         |\n",
    "| `st.text_area`, `st.slider`    | Input dell’utente                            |\n",
    "| `st.button`                    | Bottone per avviare la generazione           |\n",
    "| `st.empty()`                   | Placeholder aggiornabile per output dinamico |\n",
    "| `st.warning`, `st.error`, etc. | Feedback visivo all’utente                   |\n",
    "\n",
    "Questo approccio consente di costruire in poche righe una vera e propria **interfaccia frontend per LLM**, completamente gestita in Python, **senza usare Flask o React**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45282037-fb90-4447-8246-e8eca91d71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNZIONE ASINCRONA CON RETRY E BACKOFF -------------------------------\n",
    "\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=8),\n",
    "    stop=stop_after_attempt(4),\n",
    "    retry=retry_if_exception_type((\n",
    "        httpx.ConnectError,\n",
    "        httpx.ReadError,\n",
    "        httpx.RemoteProtocolError,\n",
    "        httpx.TimeoutException,\n",
    "        httpx.HTTPStatusError,\n",
    "    )),\n",
    "    reraise=True,\n",
    ")\n",
    "async def stream_lmstudio(prompt_text: str, temperature: float, max_tokens: int, model_name: str):\n",
    "    \"\"\"\n",
    "    Effettua una chiamata al server OpenAI-compatible di LM Studio con stream=True\n",
    "    e restituisce un async generator di 'pezzi' di testo man mano che arrivano.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": model_name or \"local-llm\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"stream\": True,  # chiave per lo streaming token-by-token\n",
    "    }\n",
    "\n",
    "    timeout = httpx.Timeout(connect=10.0, read=600.0, write=30.0, pool=10.0)\n",
    "    async with httpx.AsyncClient(timeout=timeout) as client:\n",
    "        async with client.stream(\"POST\", LMSTUDIO_URL, headers=HEADERS, json=payload) as resp:\n",
    "            # Genera HTTPStatusError se 4xx/5xx (così scatta il retry)\n",
    "            resp.raise_for_status()\n",
    "\n",
    "            # LM Studio emette SSE \"data: {...}\" + \"[DONE]\"\n",
    "            async for line in resp.aiter_lines():\n",
    "                if not line:\n",
    "                    continue\n",
    "                if not line.startswith(\"data: \"):\n",
    "                    # Alcuni server potrebbero inviare righe di keep-alive/commenti\n",
    "                    continue\n",
    "\n",
    "                data_line = line[6:].strip()\n",
    "                if data_line == \"[DONE]\":\n",
    "                    break\n",
    "\n",
    "                # Prova a decodificare JSON nel formato OpenAI \"delta\"\n",
    "                piece = None\n",
    "                try:\n",
    "                    obj = json.loads(data_line)\n",
    "                    delta = obj[\"choices\"][0].get(\"delta\", {})\n",
    "                    piece = delta.get(\"content\", \"\")\n",
    "                except Exception:\n",
    "                    # Se non è JSON valido, emetti comunque la riga grezza\n",
    "                    piece = data_line\n",
    "\n",
    "                if piece:\n",
    "                    yield piece\n",
    "\n",
    "# --- CONSUMER ASINCRONO -> AGGIORNA LA UI --------------------------------\n",
    "\n",
    "async def consume_stream_and_render(prompt_text: str, temperature: float, max_tokens: int, model_name: str):\n",
    "    placeholder = st.empty()\n",
    "    buffer = []\n",
    "\n",
    "    try:\n",
    "        async for chunk in stream_lmstudio(prompt_text, temperature, max_tokens, model_name):\n",
    "            buffer.append(chunk)\n",
    "            # Aggiornamento progressivo (Markdown per andare a capo correttamente)\n",
    "            placeholder.markdown(\"\".join(buffer))\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        # Errori 4xx/5xx non recuperabili dopo i retry\n",
    "        st.error(f\"Errore dal server LM Studio: {e.response.status_code} - {e.response.text}\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Errore di rete o streaming: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085e3bf-0db2-4fc1-bd7f-a59ba03c64fc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  **Scopo della funzione**\n",
    "\n",
    "```python\n",
    "async def stream_lmstudio(prompt_text: str, temperature: float, max_tokens: int, model_name: str)\n",
    "```\n",
    "\n",
    "La funzione è:\n",
    "\n",
    "* **asincrona** (`async`): può essere eseguita senza bloccare il thread principale.\n",
    "* Restituisce un **async generator**: `yield` viene usato per restituire **pezzi di testo man mano che arrivano**.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Decoratore `@retry(...)` con `tenacity`**\n",
    "\n",
    "```python\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=1, max=8),\n",
    "    stop=stop_after_attempt(4),\n",
    "    retry=retry_if_exception_type((...)),\n",
    "    reraise=True,\n",
    ")\n",
    "```\n",
    "\n",
    "###  Cosa fa\n",
    "\n",
    "Questa parte configura **meccanismi automatici di retry** in caso di errori di rete.\n",
    "\n",
    "| Parametro                      | Spiegazione                                                                                      |\n",
    "| ------------------------------ | ------------------------------------------------------------------------------------------------ |\n",
    "| `wait_exponential(...)`        | Implementa il **backoff esponenziale**: aspetta 1s, poi 2s, 4s, 8s tra i retry (fino a `max=8`). |\n",
    "| `stop_after_attempt(4)`        | Tenta al massimo 4 volte prima di fallire.                                                       |\n",
    "| `retry_if_exception_type(...)` | Indica quali **eccezioni triggerano il retry** (es. timeout, connessione persa, errori HTTP).    |\n",
    "| `reraise=True`                 | Se dopo i retry fallisce ancora, **rilancia l'eccezione** invece di silenziarla.                 |\n",
    "\n",
    "---\n",
    "\n",
    "##  **Payload della richiesta**\n",
    "\n",
    "```python\n",
    "payload = {\n",
    "    \"model\": model_name or \"local-llm\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt_text}],\n",
    "    \"temperature\": temperature,\n",
    "    \"max_tokens\": max_tokens,\n",
    "    \"stream\": True,\n",
    "}\n",
    "```\n",
    "\n",
    "Questo oggetto JSON segue la **specifica delle API OpenAI**. Le chiavi principali:\n",
    "\n",
    "* `model`: il nome del modello LLM da usare.\n",
    "* `messages`: array di messaggi in stile chat (ruolo + contenuto).\n",
    "* `temperature`: grado di casualità della risposta.\n",
    "* `max_tokens`: massimo numero di token generabili.\n",
    "* `stream`: `True` → abilitazione dello **streaming token-by-token**.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Timeout completo con `httpx`**\n",
    "\n",
    "```python\n",
    "timeout = httpx.Timeout(connect=10.0, read=600.0, write=30.0, pool=10.0)\n",
    "```\n",
    "\n",
    "Vengono **esplicitamente impostati tutti i timeout** per evitare l’errore che hai incontrato prima:\n",
    "\n",
    "| Campo     | Significato                                         |\n",
    "| --------- | --------------------------------------------------- |\n",
    "| `connect` | Timeout per stabilire la connessione                |\n",
    "| `read`    | Timeout massimo per leggere i dati                  |\n",
    "| `write`   | Tempo massimo per scrivere il corpo della richiesta |\n",
    "| `pool`    | Timeout per accedere a una connessione dal pool     |\n",
    "\n",
    "---\n",
    "\n",
    "##  **Streaming della risposta**\n",
    "\n",
    "```python\n",
    "async with httpx.AsyncClient(timeout=timeout) as client:\n",
    "    async with client.stream(\"POST\", LMSTUDIO_URL, headers=HEADERS, json=payload) as resp:\n",
    "        resp.raise_for_status()\n",
    "```\n",
    "\n",
    "###  Spiegazione\n",
    "\n",
    "* Viene usato `httpx.AsyncClient` per gestire una **richiesta asincrona stream**.\n",
    "* Il metodo `client.stream(...)` restituisce una connessione dove puoi leggere le righe **man mano che arrivano**.\n",
    "* `raise_for_status()` lancia eccezione se lo status è 4xx/5xx → permette il retry.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Parsing dello stream SSE (Server-Sent Events)**\n",
    "\n",
    "```python\n",
    "async for line in resp.aiter_lines():\n",
    "    if not line or not line.startswith(\"data: \"): continue\n",
    "    data_line = line[6:].strip()\n",
    "    if data_line == \"[DONE]\": break\n",
    "```\n",
    "\n",
    "###  Cos'è\n",
    "\n",
    "* LM Studio segue il formato **Server-Sent Events**, quindi ogni linea è prefissata con `\"data: \"` e termina con `[DONE]`.\n",
    "* Il codice:\n",
    "\n",
    "  * ignora righe vuote o non prefissate da `data:`\n",
    "  * esce dal ciclo se trova `[DONE]`\n",
    "\n",
    "---\n",
    "\n",
    "##  **Parsing JSON della risposta**\n",
    "\n",
    "```python\n",
    "obj = json.loads(data_line)\n",
    "delta = obj[\"choices\"][0].get(\"delta\", {})\n",
    "piece = delta.get(\"content\", \"\")\n",
    "```\n",
    "\n",
    "###  Cos'è\n",
    "\n",
    "* La risposta `data_line` è un oggetto JSON che contiene un campo `\"delta\"` con i nuovi token generati.\n",
    "* `delta[\"content\"]` contiene il nuovo pezzo di testo.\n",
    "* Se non è presente o se il JSON è malformato, viene emessa comunque la riga grezza.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Yield progressivo**\n",
    "\n",
    "```python\n",
    "if piece:\n",
    "    yield piece\n",
    "```\n",
    "\n",
    "Questo è ciò che **permette lo streaming reale**. Il `yield` restituisce un token alla volta (o blocco) man mano che arriva.\n",
    "\n",
    "---\n",
    "\n",
    "##  Come viene usata questa funzione\n",
    "\n",
    "La funzione viene usata in:\n",
    "\n",
    "```python\n",
    "async for chunk in stream_lmstudio(...):\n",
    "    buffer.append(chunk)\n",
    "    placeholder.markdown(\"\".join(buffer))\n",
    "```\n",
    "\n",
    " Questo aggiorna l’UI Streamlit in tempo reale con il testo generato.\n",
    "\n",
    "---\n",
    "\n",
    "##  In sintesi\n",
    "\n",
    "| Aspetto                  | Funzione                                         |\n",
    "| ------------------------ | ------------------------------------------------ |\n",
    "| **Decoratore `@retry`**  | Garantisce robustezza alle chiamate API          |\n",
    "| **`async` + `yield`**    | Rende la funzione asincrona e progressiva        |\n",
    "| **`httpx.stream(...)`**  | Riceve output token-by-token dal modello         |\n",
    "| **Parsing SSE + JSON**   | Interpreta correttamente il flusso in arrivo     |\n",
    "| **Gestione errori**      | Rilancia solo errori seri dopo i retry           |\n",
    "| **Compatibilità OpenAI** | La struttura è identica alle API OpenAI standard |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da85e1-8512-4631-bd05-7428a7b6e751",
   "metadata": {},
   "source": [
    "## AZURE OPENAI API ##\n",
    "\n",
    "---\n",
    "\n",
    "### requirements.txt\n",
    "\n",
    "```txt\n",
    "streamlit>=1.30.0\n",
    "openai>=1.40.0\n",
    "python-dotenv>=1.0.1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### app.py\n",
    "\n",
    "```python\n",
    "# ================================\n",
    "# Streamlit + Azure OpenAI (Streaming + Multi-turn)\n",
    "# ================================\n",
    "\n",
    "# --- Import standard library ---\n",
    "import os  # per leggere variabili d'ambiente (endpoint, chiave, versione API)\n",
    "from typing import List, Dict  # typing facoltativo per chiarezza del codice\n",
    "\n",
    "# --- Import di terze parti ---\n",
    "from dotenv import load_dotenv  # carica automaticamente variabili da .env (comodo in locale)\n",
    "import streamlit as st          # framework UI in Python\n",
    "from openai import AzureOpenAI  # client ufficiale OpenAI compatibile con Azure\n",
    "\n",
    "# ================================\n",
    "# Configurazione iniziale dell'app\n",
    "# ================================\n",
    "\n",
    "# Carichiamo eventuali variabili dal file .env nella sessione (utile in sviluppo locale)\n",
    "# Il file .env (non committare su git) può contenere:\n",
    "#   AZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com/\n",
    "#   AZURE_OPENAI_API_VERSION=2024-12-01-preview\n",
    "#   AZURE_OPENAI_KEY=<your-api-key>\n",
    "#   AZURE_OPENAI_DEPLOYMENT=<deployment-name>   (nome del deployment chat su Azure)\n",
    "load_dotenv()\n",
    "\n",
    "# Impostazioni di pagina: titolo e layout\n",
    "st.set_page_config(page_title=\"Azure OpenAI – Streaming Chat\", layout=\"wide\")\n",
    "\n",
    "# Titolo e descrizione della pagina\n",
    "st.title(\"Azure OpenAI – Streaming Chat (Streamlit)\")\n",
    "st.write(\n",
    "    \"Demo di chat multi-turno con **streaming** via Azure OpenAI. \"\n",
    "    \"Configura i parametri nella sidebar, inserisci un messaggio e invia.\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Sidebar: configurazione runtime\n",
    "# ================================\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Configurazione Azure\")\n",
    "    # Leggiamo le variabili d'ambiente (se non esistono, mostriamo placeholder)\n",
    "    endpoint = st.text_input(\n",
    "        \"Endpoint\",\n",
    "        value=os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"https://<resource>.openai.azure.com/\"),\n",
    "        help=\"URL del servizio Azure OpenAI (Sezione 'Chiavi ed endpoint').\",\n",
    "    )\n",
    "    api_version = st.text_input(\n",
    "        \"API Version\",\n",
    "        value=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\"),\n",
    "        help=\"Versione API supportata dalla tua risorsa.\",\n",
    "    )\n",
    "    api_key = st.text_input(\n",
    "        \"API Key\",\n",
    "        value=os.getenv(\"AZURE_OPENAI_KEY\", \"\"),\n",
    "        type=\"password\",\n",
    "        help=\"Chiave di accesso al servizio (non salvarla in chiaro in produzione).\",\n",
    "    )\n",
    "    deployment = st.text_input(\n",
    "        \"Deployment name (chat)\",\n",
    "        value=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-35-turbo\"),\n",
    "        help=\"Nome del deployment chat creato nel portale Azure.\",\n",
    "    )\n",
    "\n",
    "    st.divider()\n",
    "    st.header(\"Parametri generazione\")\n",
    "    temperature = st.slider(\"temperature\", 0.0, 2.0, 0.7, 0.1)\n",
    "    top_p = st.slider(\"top_p\", 0.0, 1.0, 1.0, 0.05)\n",
    "    max_tokens = st.number_input(\"max_tokens\", min_value=16, max_value=4096, value=512, step=16)\n",
    "\n",
    "    st.divider()\n",
    "    st.header(\"System prompt\")\n",
    "    # Prompt di sistema opzionale per condizionare il comportamento del modello\n",
    "    default_system = \"You are a helpful assistant. Answer clearly and concisely.\"\n",
    "    system_prompt = st.text_area(\"Contenuto system\", height=100, value=default_system)\n",
    "\n",
    "    # Pulsanti di utilità: reset conversazione\n",
    "    reset = st.button(\"🔄 Reset conversazione\", use_container_width=True)\n",
    "\n",
    "# ================================\n",
    "# Inizializzazione stato conversazione\n",
    "# ================================\n",
    "\n",
    "# Streamlit riesegue lo script ad ogni interazione: usiamo session_state per preservare lo stato.\n",
    "if \"messages\" not in st.session_state or reset:\n",
    "    # Ogni messaggio è un dict {'role': 'system'|'user'|'assistant', 'content': '<testo>'}\n",
    "    st.session_state.messages: List[Dict[str, str]] = []\n",
    "    # Inseriamo il messaggio di sistema (se non vuoto) in testa alla conversazione\n",
    "    if system_prompt.strip():\n",
    "        st.session_state.messages.append({\"role\": \"system\", \"content\": system_prompt.strip()})\n",
    "\n",
    "# ================================\n",
    "# Helper: istanziare il client AzureOpenAI\n",
    "# ================================\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def get_azure_client(_endpoint: str, _api_key: str, _api_version: str) -> AzureOpenAI:\n",
    "    \"\"\"\n",
    "    Crea e ritorna un client AzureOpenAI.\n",
    "    - @st.cache_resource: evita di ricreare il client ad ogni rerun (fino a cambio parametri).\n",
    "    \"\"\"\n",
    "    # Validazione minima: endpoint e key devono essere plausibili\n",
    "    if not _endpoint or not _endpoint.startswith(\"https://\"):\n",
    "        raise ValueError(\"Endpoint Azure non valido. Controlla la sidebar.\")\n",
    "    if not _api_key:\n",
    "        raise ValueError(\"API Key mancante. Inseriscila nella sidebar.\")\n",
    "\n",
    "    # AzureOpenAI richiede:\n",
    "    #   - azure_endpoint: URL della risorsa Azure OpenAI\n",
    "    #   - api_key: chiave\n",
    "    #   - api_version: versione API supportata dalla risorsa\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=_endpoint,\n",
    "        api_key=_api_key,\n",
    "        api_version=_api_version,\n",
    "    )\n",
    "    return client\n",
    "\n",
    "# Proviamo a creare il client (se fallisce, mostriamo errore e stop)\n",
    "try:\n",
    "    client = get_azure_client(endpoint, api_key, api_version)\n",
    "except Exception as e:\n",
    "    st.error(f\"Errore inizializzazione client AzureOpenAI: {e}\")\n",
    "    st.stop()\n",
    "\n",
    "# ================================\n",
    "# Sezione chat: visualizzazione storico\n",
    "# ================================\n",
    "\n",
    "# Mostriamo i messaggi già scambiati (escludiamo il 'system' dalla UI, ma lo manteniamo nel contesto)\n",
    "for msg in st.session_state.messages:\n",
    "    if msg[\"role\"] == \"system\":\n",
    "        continue  # non visualizziamo il system nella chat\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "# ================================\n",
    "# Input utente (chat)\n",
    "# ================================\n",
    "\n",
    "# Componente nativo chat (render moderno): ritorna stringa quando l'utente invia\n",
    "user_input = st.chat_input(\"Scrivi un messaggio e premi Invio...\")\n",
    "\n",
    "# Se l'utente invia un messaggio:\n",
    "if user_input:\n",
    "    # 1) Appendiamo subito il messaggio dell'utente allo storico\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # 2) Mostriamo il messaggio nella UI (subito, senza aspettare la risposta)\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "\n",
    "    # 3) Creiamo lo \"slot\" per la risposta dell'assistente con un placeholder\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        # Questo placeholder verrà aggiornato progressivamente durante lo streaming\n",
    "        stream_area = st.empty()\n",
    "\n",
    "        # 4) Prepariamo la struttura dei messaggi per la chiamata Azure (includendo il system)\n",
    "        #    Nota: Azure usa lo schema OpenAI \"chat.completions\", dove 'messages' è una lista di turni.\n",
    "        messages_payload = st.session_state.messages\n",
    "\n",
    "        # 5) Eseguiamo la chiamata in streaming\n",
    "        try:\n",
    "            # La chiamata al client AzureOpenAI: chat.completions.create\n",
    "            # - stream=True attiva l'invio dei chunk progressivi\n",
    "            # - model=<deployment> è il nome del deployment Azure (non il \"nome modello\" puro)\n",
    "            stream = client.chat.completions.create(\n",
    "                model=deployment,               # nome del deployment Azure (es. \"gpt-35-turbo\")\n",
    "                messages=messages_payload,      # contesto completo: system+storico+nuovo turno\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=True,                    # fondamentale per ricevere chunk incrementali\n",
    "            )\n",
    "\n",
    "            # 6) Iteriamo i \"chunk\" dello stream e aggiorniamo la UI in tempo reale\n",
    "            full_response_text = \"\"  # accumulatore testo generato\n",
    "            for update in stream:\n",
    "                # La struttura è in stile OpenAI: choices[0].delta.content contiene il delta testuale\n",
    "                if update.choices:\n",
    "                    delta = update.choices[0].delta\n",
    "                    # Il primo chunk spesso contiene solo \"role\", NON un contenuto\n",
    "                    piece = getattr(delta, \"content\", None)\n",
    "                    if piece:\n",
    "                        full_response_text += piece\n",
    "                        # Aggiorniamo il box con il testo accumulato (effetto \"digitazione\")\n",
    "                        stream_area.markdown(full_response_text)\n",
    "\n",
    "            # 7) Terminato lo stream, salviamo la risposta completa nello storico\n",
    "            st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response_text})\n",
    "\n",
    "        except Exception as e:\n",
    "            # Qualsiasi errore di rete/permessi/limiti viene mostrato qui\n",
    "            stream_area.error(f\"Errore durante lo streaming: {e}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Come usare\n",
    "\n",
    "1. Imposta le variabili d’ambiente (opzionale, consigliato in `.env`):\n",
    "\n",
    "   ```env\n",
    "   AZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com/\n",
    "   AZURE_OPENAI_API_VERSION=2024-12-01-preview\n",
    "   AZURE_OPENAI_KEY=<your-api-key>\n",
    "   AZURE_OPENAI_DEPLOYMENT=gpt-35-turbo\n",
    "   ```\n",
    "2. Installa i pacchetti:\n",
    "\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "3. Avvia l’app:\n",
    "\n",
    "   ```bash\n",
    "   streamlit run app.py\n",
    "   ```\n",
    "4. Inserisci eventuali parametri mancanti nella **sidebar**, poi scrivi un messaggio nella **chat**.\n",
    "5. Guarda la risposta arrivare **token per token** e continua il dialogo: la cronologia è mantenuta in `st.session_state.messages`.\n",
    "\n",
    "---\n",
    "\n",
    "### Note tecniche importanti\n",
    "\n",
    "* **model=deployment**: su Azure devi passare il **nome del deployment** (es. `gpt-35-turbo-2`), non il puro nome del modello.\n",
    "* **Streaming**: l’iterazione `for update in stream:` restituisce frammenti dove il testo nuovo è in `update.choices[0].delta.content`.\n",
    "* **System prompt**: è aggiunto come primo messaggio in `session_state.messages` e non viene mostrato nella UI.\n",
    "* **Cache del client**: `@st.cache_resource` evita di ricreare il client a ogni rerun; si invalida quando cambiano i parametri.\n",
    "* **Sicurezza**: non committare chiavi o `.env`; in produzione usa Key Vault/Secrets Manager.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93f873-a1a0-4f08-8e9a-99cb7b5fcda2",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🤗 **PARTE 6 – Hugging Face Inference Endpoints**\n",
    "\n",
    "### 6.1 Inference API vs Endpoint Dedicato\n",
    "\n",
    "---\n",
    "\n",
    "###  **Cos’è l’Inference API pubblica?**\n",
    "\n",
    "L’Inference API di Hugging Face è un servizio **chiavi in mano** che ti permette di usare un modello pre-addestrato **senza doverlo ospitare** o gestire.\n",
    "È pensata per:\n",
    "\n",
    "* test veloci\n",
    "* prototipazione\n",
    "* accesso gratuito o limitato per sviluppatori\n",
    "\n",
    "####  Vantaggi:\n",
    "\n",
    "* Nessuna configurazione necessaria\n",
    "* Migliaia di modelli disponibili\n",
    "* Gratuita con limiti o piani a consumo\n",
    "\n",
    "####  Svantaggi:\n",
    "\n",
    "* **Rate limit** basso e shared server\n",
    "* **Latenza alta** per modelli grandi\n",
    "* Nessun controllo su scalabilità o versione\n",
    "* **Non adatta a produzione**\n",
    "\n",
    " **Esempio di utilizzo:**\n",
    "\n",
    "```bash\n",
    "curl https://api-inference.huggingface.co/models/gpt2 \\\n",
    "  -H \"Authorization: Bearer YOUR_HF_TOKEN\" \\\n",
    "  -d '{\"inputs\": \"Ciao, come stai?\"}'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  **Cos’è un Endpoint Dedicato?**\n",
    "\n",
    "Gli **Inference Endpoints** dedicati (Managed Inference Endpoints) sono modelli **deployati e serviti in modo dedicato**, su una **VM esclusiva**, configurata da Hugging Face.\n",
    "Puoi scegliere:\n",
    "\n",
    "* modello\n",
    "* framework (Transformers, TGI, TGI + PEFT, ONNX, etc.)\n",
    "* dimensione dell’istanza\n",
    "* auto-scaling, autosuspend, versione del modello\n",
    "\n",
    "####  Vantaggi:\n",
    "\n",
    "* Performance stabili e rapide\n",
    "* Nessun limite di chiamata (solo limiti di throughput)\n",
    "* Sicurezza maggiore (es. IP whitelisting)\n",
    "* Ottimo per **produzione** e **API commerciali**\n",
    "\n",
    "####  Svantaggi:\n",
    "\n",
    "* **Costo mensile fisso** (es. da \\~0.06\\$/h a >1\\$/h)\n",
    "* Richiede setup iniziale via UI o API\n",
    "* Tempi di warm-up se in autosuspend\n",
    "\n",
    " **Esempio di endpoint**:\n",
    "\n",
    "```\n",
    "POST https://<org>.hf.space/inference/<deployment>\n",
    "Authorization: Bearer <token>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Tabella Riepilogativa\n",
    "\n",
    "| Feature                 | Inference API Pubblica | Endpoint Dedicato               |\n",
    "| ----------------------- | ---------------------- | ------------------------------- |\n",
    "| **Accesso**             | Gratuito o token HF    | Privato, a pagamento            |\n",
    "| **Performance**         | Variabile              | Costante                        |\n",
    "| **Scalabilità**         | Nessuna                | Auto-scaling disponibile        |\n",
    "| **Sicurezza**           | Nessuna                | IP allowlist, token, rate limit |\n",
    "| **Adatto a produzione** | No                      | Si                               |\n",
    "| **Configurazione**      | Zero-config            | Manuale o via API               |\n",
    "\n",
    "---\n",
    "\n",
    "###  Quando scegliere cosa?\n",
    "\n",
    "| Scenario                         | Soluzione consigliata                                              |\n",
    "| -------------------------------- | ------------------------------------------------------------------ |\n",
    "| Vuoi testare un modello        | Inference API pubblica                                             |\n",
    "| Hai un MVP/POC leggero         | Inference API o Endpoint su istanza piccola                        |\n",
    "| Hai un prodotto in produzione  | Endpoint dedicato con auto-scaling                                 |\n",
    "| Vuoi ospitare in locale       | Self-deployment con `transformers`, TGI, o `text-generation-webui` |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad49a42-0f2f-4cf2-977e-710af48faa09",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6.2 Autoscaling – Hugging Face Inference Endpoints\n",
    "\n",
    "### Cos’è l’autoscaling\n",
    "\n",
    "L’autoscaling nei **Hugging Face Inference Endpoints** è una funzionalità che permette all’infrastruttura di adattarsi automaticamente al carico delle richieste.\n",
    "In base al traffico in ingresso, Hugging Face può:\n",
    "\n",
    "* avviare istanze aggiuntive quando ci sono molte richieste (scaling out)\n",
    "* sospendere o ridurre le istanze quando non ci sono richieste (scaling in)\n",
    "\n",
    "Questa strategia è utile per **ottimizzare i costi**, evitando di mantenere attive risorse inutilizzate, e allo stesso tempo **garantire la disponibilità** quando il traffico aumenta.\n",
    "\n",
    "---\n",
    "\n",
    "### Cold start e warm instances\n",
    "\n",
    "#### Warm instance\n",
    "\n",
    "Una **warm instance** è un’istanza già attiva e pronta a rispondere.\n",
    "La latenza della prima risposta è bassa, in quanto il modello è già caricato in memoria.\n",
    "\n",
    "#### Cold start\n",
    "\n",
    "Quando un endpoint non riceve richieste per un certo periodo (tipicamente 5-10 minuti), Hugging Face può metterlo in **autosuspend**.\n",
    "Alla successiva richiesta, viene effettuato un **cold start**, ovvero:\n",
    "\n",
    "* l’istanza viene avviata\n",
    "* il modello viene ricaricato in memoria\n",
    "* la prima risposta può richiedere **decine di secondi**\n",
    "\n",
    "Questo comportamento è simile a quanto avviene nei serverless functions.\n",
    "\n",
    "#### Come evitarlo\n",
    "\n",
    "Per mantenere le istanze sempre attive:\n",
    "\n",
    "* è possibile **disattivare l’autosuspend** (aumentando i costi)\n",
    "* oppure inviare periodicamente una chiamata di **keep-alive**\n",
    "\n",
    "---\n",
    "\n",
    "### Come dimensionare correttamente per il proprio carico\n",
    "\n",
    "Per dimensionare correttamente un endpoint con autoscaling, bisogna considerare:\n",
    "\n",
    "1. **Tempo medio di inferenza**\n",
    "\n",
    "   * Modelli piccoli (es. distilBERT) possono rispondere in <1s\n",
    "   * Modelli grandi (es. LLaMA 2 13B) possono impiegare diversi secondi o minuti per risposte lunghe\n",
    "\n",
    "2. **Concorrenza**\n",
    "\n",
    "   * Quanti utenti chiamano l’API in parallelo\n",
    "   * Hugging Face supporta la definizione di un **batch size** o **max concurrency**\n",
    "\n",
    "3. **Ritmo di richieste**\n",
    "\n",
    "   * Se le richieste arrivano in burst (es. campagne), servono più istanze\n",
    "   * Se le richieste sono regolari, si può usare una singola istanza con autoscaling minimo a 1\n",
    "\n",
    "4. **Budget**\n",
    "\n",
    "   * Ogni istanza ha un costo orario\n",
    "   * Puoi impostare **limiti massimi di istanze** per non superare un certo budget\n",
    "\n",
    "5. **Strategia consigliata**\n",
    "\n",
    "| Fase            | Configurazione consigliata                                                |\n",
    "| --------------- | ------------------------------------------------------------------------- |\n",
    "| Sviluppo o test | 1 istanza, autosuspend attivo                                             |\n",
    "| MVP / beta      | 1-2 istanze, autoscaling attivo, warmup periodico                         |\n",
    "| Produzione      | Min 1 istanza warm, autoscaling 1–5, autosuspend disattivato o keep-alive |\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusione\n",
    "\n",
    "L’autoscaling di Hugging Face consente una gestione flessibile delle risorse in base al traffico. Tuttavia, è importante conoscere il comportamento del cold start per evitare penalizzazioni di latenza in ambienti critici.\n",
    "Un’analisi accurata del carico atteso e una buona configurazione dell’endpoint permettono di bilanciare efficacemente performance e costi.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd4f95-254a-402a-9f85-d404265c8d7c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6.3 Esempio con Hugging Face\n",
    "\n",
    "### Installazione librerie necessarie\n",
    "\n",
    "Per usare Hugging Face sia in locale che via API:\n",
    "\n",
    "```bash\n",
    "pip install requests transformers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Parte 1 – Chiamata REST a un modello Hugging Face Inference Endpoint\n",
    "\n",
    "> Questa modalità richiede un **token di accesso personale** da Hugging Face e funziona **senza scaricare i modelli localmente**.\n",
    "\n",
    "### 1.1 Ottenere il token\n",
    "\n",
    "* Vai su [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "* Crea un nuovo token con scope `read`\n",
    "* Salvalo come variabile d’ambiente o usalo direttamente nel codice\n",
    "\n",
    "### 1.2 Esempio chiamata API REST\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"  # puoi cambiare modello\n",
    "HF_TOKEN = \"Bearer hf_...\"  # il tuo token personale\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": HF_TOKEN\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"inputs\": \"The future of AI is\",\n",
    "    \"parameters\": {\"max_new_tokens\": 50, \"temperature\": 0.7},\n",
    "}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json=data)\n",
    "\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "#### Output atteso\n",
    "\n",
    "L’output sarà un JSON con i completamenti generati.\n",
    "Per esempio:\n",
    "\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"generated_text\": \"The future of AI is exciting and full of possibilities...\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Puoi usare qualsiasi modello pubblico o privato di Hugging Face, basta sostituire `gpt2` nell’URL con il nome del modello (es. `\"HuggingFaceH4/zephyr-7b-beta\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## Parte 2 – Uso locale con `pipeline()` di `transformers`\n",
    "\n",
    "> Questa modalità scarica il modello **in locale** (richiede RAM e spazio disco), utile per ambienti offline o se non vuoi usare le API.\n",
    "\n",
    "### 2.1 Esempio locale con `pipeline()`\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Scarica e carica GPT-2 in locale\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Prompt da completare\n",
    "prompt = \"The future of AI is\"\n",
    "\n",
    "# Generazione\n",
    "output = generator(prompt, max_new_tokens=50, temperature=0.7)\n",
    "\n",
    "print(output[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "Questo esempio:\n",
    "\n",
    "* Usa la pipeline di **text-generation**\n",
    "* Scarica il modello se non già presente\n",
    "* Esegue tutto **senza connessione Internet** dopo il primo download\n",
    "\n",
    "---\n",
    "\n",
    "## Confronto tra API e pipeline locale\n",
    "\n",
    "| Aspetto      | Inference API Hugging Face         | `pipeline()` locale                    |\n",
    "| ------------ | ---------------------------------- | -------------------------------------- |\n",
    "| Setup        | Solo token e internet              | Richiede download del modello          |\n",
    "| Latenza      | Più alta (cloud)                   | Più bassa (se hardware adeguato)       |\n",
    "| Costo        | Basato su token/uso                | Gratuito dopo download                 |\n",
    "| Flessibilità | Limitata (solo modelli pubblicati) | Totale (puoi modificare e ottimizzare) |\n",
    "| Scalabilità  | Molto alta (gestita da HF)         | Limitata all’hardware locale           |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d624e67-f258-4989-bfc9-223f9fa3cf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
